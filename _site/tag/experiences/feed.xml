<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator>
  <link href="/tag/experiences/feed.xml" rel="self" type="application/atom+xml" />
  <link href="/" rel="alternate" type="text/html" />
  <updated>2023-08-06T22:18:22-07:00</updated>
  <id>/tag/experiences/feed.xml</id>

  
  
  

  
    <title type="html">Atharva Pusalkar | </title>
  

  
    <subtitle>Atharva Pusalkar</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Google Summer of Code at Open Robotics</title>
      <link href="/gsoc" rel="alternate" type="text/html" title="Google Summer of Code at Open Robotics" />
      <published>2021-08-22T03:18:00-07:00</published>
      <updated>2021-08-22T03:18:00-07:00</updated>
      <id>/gsoc</id>
      <content type="html" xml:base="/gsoc">&lt;p&gt;In the summer of 2021, I had the opportunity to work at Open Robotics (n√©e
Open Source Robotics Foundation) as a student developer. The project was in turn funded by Google through their Summer of Code 2021 program.&lt;/p&gt;

&lt;p&gt;Back in late 2020, I was looking at new open-source projects that I could contribute to, and by good fortune I stumbled upon the Ignition Robotics project. I was already a wee bit familiar with some of the software published by them and by Open Robotics at large - one of them being the Gazebo robotics simulator. I actively started contributing to their project by making small pull requests at first, such as adding an &lt;i&gt;about dialog&lt;/i&gt;, light intensity modification, and more. Three months later, I applied to the Summer of Code 2021 program at Open Robotics, and one fine day, after a video meeting and evaluation, I got the acceptance mail!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gsoc_acceptance.jpg&quot; alt=&quot;Acceptance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to read the blog post by Open Robotics, visit &lt;a href=&quot;https://www.openrobotics.org/blog/2021/5/25/google-summer-of-code-2021&quot;&gt;https://www.openrobotics.org/blog/2021/5/25/google-summer-of-code-202&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Watch and hear me talk about the debugging features added to Ignition Gazebo in the Fortress release demo meeting:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/VwzGJDHkW8A?start=1009&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;the-project&quot;&gt;The Project&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/gsoc_open_robotics.png&quot; alt=&quot;&amp;quot;GSoC&amp;quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the past, developing simulations worlds was a trial and error process, and users often had to provide computed values for robot inertia, joints, and mass properties, and hope that they will work in the simulation directly. If the physical behavior of the robot differed from what the user expected, there was no way to visualize what went wrong in the simulation world.&lt;/p&gt;

&lt;p&gt;Due to the growing user base of the newer simulator, with it finding its applications in the industry and competitions such as the DARPA SubT challenge, it was becoming imperative to add capabilities to visualize values coming from the physics engine of Gazebo. Hence, this project was undertaken to add new features to visualize the robot inertia, mass, joints, and more at runtime.&lt;/p&gt;

&lt;h2 id=&quot;project-breakdown&quot;&gt;Project Breakdown&lt;/h2&gt;

&lt;p&gt;This project involves working on two Ignition libraries - Gazebo and Rendering. The visual classes are first added to the Rendering library which is then consumed by the downstream Gazebo project. A total of 9 pull requests were made during this project:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pulls?q=is%3Apr+author%3A%22atharva-18%22+created%3A%3E2021-05-01+closed%3A%3C2021-10-01++is%3Aclosed+&quot;&gt;Ignition Gazebo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ignitionrobotics/ign-rendering/pulls?q=is%3Apr+author%3A%22atharva-18%22+created%3A%3E2021-05-01+closed%3A%3C2021-10-01++is%3Aclosed+&quot;&gt;Ignition Rendering&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since new visuals are first added to the Ignition Rendering library, this project also extends Ignition Rendering to have a visualization component along with sensors and cameras (Please refer to this video &lt;a href=&quot;https://youtu.be/JugZs9rzpKM?t=1227&quot;&gt;Ignition vs Gazebo Classic + Ignition Rendering: Community Meeting&lt;/a&gt; at 20:27 to get an idea about the architecture).&lt;/p&gt;

&lt;p&gt;These visualizations primarily target the Ogre 1.9 and Ogre 2.2 render engines as guided by the Ignition Gazebo architecture. That being said, the joint visualization should work for the NVIDIA OptiX raytracing engine as well. Currently, Ubuntu 18.04 and 20.04 along with Windows 10 and above are supported. Support for macOS is not under development due to additional requirements for the Metal API.&lt;/p&gt;

&lt;h3 id=&quot;visualize-as-wireframe&quot;&gt;Visualize as Wireframe&lt;/h3&gt;

&lt;p&gt;Users often need to debug their 3D meshes, hence we added a wireframe rendering mode to visuals, similar to Gazebo Classic&lt;/p&gt;

&lt;p&gt;Ignition Rendering Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rendering/pull/314&quot;&gt;https://github.com/ignitionrobotics/ign-rendering/pull/314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/816&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/816&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/wireframe.gif&quot; alt=&quot;Wireframe&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualize-as-transparent&quot;&gt;Visualize as Transparent&lt;/h3&gt;

&lt;p&gt;You can now make a robot model or its link transparent in Gazebo, to help you see its other aspects more clearly.&lt;/p&gt;

&lt;p&gt;Ignition Gazebo Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/878&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/878&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/transparent.gif&quot; alt=&quot;Transparent&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualize-inertia&quot;&gt;Visualize Inertia&lt;/h3&gt;

&lt;p&gt;Visualize the inertia of a robot model or link using its physics inertia properties (No more adding values in SDF files blindly).&lt;/p&gt;

&lt;p&gt;Ignition Rendering Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rendering/pull/326&quot;&gt;https://github.com/ignitionrobotics/ign-rendering/pull/326&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/861&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Rendering:
&lt;img src=&quot;assets/images/gsoc_inertia_rendering.png&quot; alt=&quot;Inertia&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo:
Here‚Äôs the inertia visualization in different scales and orientations:
&lt;img src=&quot;assets/images/inertia.png&quot; alt=&quot;Inertia&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualize-center-of-mass&quot;&gt;Visualize Center of Mass&lt;/h3&gt;

&lt;p&gt;Visualize the center of mass of a robot model or link, using the same inertia values.&lt;/p&gt;

&lt;p&gt;Ignition Rendering Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rendering/pull/345&quot;&gt;https://github.com/ignitionrobotics/ign-rendering/pull/345&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/903&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/903&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Rendering:
&lt;img src=&quot;assets/images/gsoc_com_rendering.png&quot; alt=&quot;CoM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo:
&lt;img src=&quot;assets/images/com.gif&quot; alt=&quot;CoM&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualize-joints&quot;&gt;Visualize Joints&lt;/h3&gt;

&lt;p&gt;You can now visualize the joints of a robot while simulating or building it. Joints such as revolute, prismatic, universal, ball, and more are supported.&lt;/p&gt;

&lt;p&gt;This visualization will be extended in the future to planned features such as the model and joint editor.&lt;/p&gt;

&lt;p&gt;Ignition Rendering Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rendering/pull/366&quot;&gt;https://github.com/ignitionrobotics/ign-rendering/pull/366&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo Pull Request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/961&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/961&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ignition Rendering:
&lt;img src=&quot;assets/images/gsoc_joint_rendering.png&quot; alt=&quot;Joints&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ignition Gazebo:
&lt;img src=&quot;assets/images/joints.gif&quot; alt=&quot;Joints&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Besides the above-mentioned, I have also added features such as the &lt;i&gt;about dialog&lt;/i&gt; and light intensity fields during the pre-GSOC period.&lt;/p&gt;

&lt;p&gt;We hope that this project will provide a better user experience to existing users and bring more people under the Ignition project by reducing the feature gap between the two simulators.&lt;/p&gt;

&lt;p&gt;These new visualizations will be released to the public with Ignition Fortress in September 2021.&lt;/p&gt;

&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;

&lt;p&gt;You can test the new features by installing Ignition Fortress from Nightly builds and using the demo world provided in this repository.&lt;/p&gt;

&lt;p&gt;After downloading, you can run the simulation by&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ign gazebo -v 4 visualization_demo.sdf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ignition Gazebo demo using a SubT vehicle:
&lt;img src=&quot;assets/images/testing.gif&quot; alt=&quot;Testing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ignition Rendering demo:
&lt;img src=&quot;assets/images/rendering-demo.png&quot; alt=&quot;Rendering Demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A small demo involving the visualization component of Ignition Rendering is now included with the source code starting with the Fortress release.&lt;/p&gt;

&lt;p&gt;As opposed to the Gazebo Classic simulator, the new Ignition libraries offer a decoupled architecture. Each component of Gazebo (For example, rendering, transport and physics) is now a separate library. The advantage is that developers can use the above-mentioned visuals for their own separate projects without having the overhead of unwanted Gazebo code.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;I thank my mentor, Alejandro Hern√°ndez Cordero, for supporting and guiding me during the entire program and before it. Thanks to the whole Open Robotics team for providing me with this opportunity, reviewing my work, and assisting me with the project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/meet.png&quot; alt=&quot;Meet with Open Robotics development team&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The non-programming part was just as much of an experience, if not more. The weekly team meetings were a great way of looking into project management and workflows. It was definitely worth staying up late at 1 AM in the night, working with people based in different timezones. Now, I am looking forward to giving my project presentation at the Ignition Fortress Demos Community Meeting.&lt;/p&gt;

&lt;p&gt;The work has helped me improve my programming skills, and I learned a great deal about 3D rendering and physics simulation.&lt;/p&gt;

&lt;h2 id=&quot;about-me&quot;&gt;About Me&lt;/h2&gt;

&lt;p&gt;I am a final-year undergraduate engineering student at DJ Sanghvi College of Engineering, University of Mumbai pursuing Electronics Engineering.&lt;/p&gt;

&lt;p&gt;I primarily work with autonomous robotics and self-driving software. I have been a part of the Formula Student team of my institute where we are building a self-developed autonomous race car. Through the team, I gained hands-on experience in robot navigation and perception and motivated me to further explore ROS and Gazebo. I have also worked at a startup to develop and deploy autonomous navigation software for use in warehouse environments.&lt;/p&gt;

&lt;p&gt;During my time at Open Robotics, I had the opportunity to directly work on features that were being used by people in the robotics industry and academia.&lt;/p&gt;

&lt;p&gt;Thank you for reading.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Experiences" />
      

      
        <summary type="html">In the summer of 2021, I had the opportunity to work at Open Robotics (n√©e Open Source Robotics Foundation) as a student developer. The project was in turn funded by Google through their Summer of Code 2021 program.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">IEEE Student Branch, DJSCE</title>
      <link href="/ieee-sb" rel="alternate" type="text/html" title="IEEE Student Branch, DJSCE" />
      <published>2021-07-05T03:18:00-07:00</published>
      <updated>2021-07-05T03:18:00-07:00</updated>
      <id>/ieee-sb</id>
      <content type="html" xml:base="/ieee-sb">&lt;p&gt;For two years of my undergrad, I was a member of the IEEE student chapter of my institute, DJ Sanghvi College of Engineering.&lt;/p&gt;

&lt;p&gt;IEEE Brainwaves is the official IEEE student branch of Dwarkadas J. Sanghvi College of Engineering. The student chapter is responsible for coordinating &amp;amp; conducting various workshops and other activities at the institute, which include expert lectures, seminars, workshops, and industry visits, and mentoring sessions by alumni. These all are the initiatives by the department to ensure a holistic learning experience in the 4 years of engineering at DJSCE.&lt;/p&gt;

&lt;p&gt;Visit out website - &lt;a href=&quot;https://www.ieeedjsce.com&quot;&gt;https://www.ieeedjsce.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I joined the team as a technical team member back in 2019 - my sophomore year. I was then promoted as the Chairperson for the following academic year. I look back at the past two years spent at the club fondly. The work and topics ranged from photo editing, electronics, programming, to even the equity markets.&lt;/p&gt;

&lt;p&gt;Here are a few glimpses of our events during the last year.&lt;/p&gt;

&lt;h2 id=&quot;drone-federation-of-india-seminar&quot;&gt;Drone Federation of India Seminar:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/DFI_1.jpg&quot; alt=&quot;DFI&quot; /&gt;
Workshop organized in collaboration with the Drone Federation of India. It introduced the world of drones including ArduPilot, navigation, and autonomy. The speaker of the event was Mr. Smit Shah who is the Director of the Drone Federation of India. The key takeaway from this seminar was the growing prominence of drone technology and its expansion in the industry. The webinar was followed by a short QnA.&lt;/p&gt;

&lt;h2 id=&quot;ieee-alumni-series-episode-1-can-protocol-and-implementation-of-rtos&quot;&gt;IEEE Alumni Series Episode 1: CAN Protocol and Implementation of RTOS:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/CAN_Protocol_Poster.png&quot; /&gt;
&lt;img src=&quot;assets/images/CAN_Protocol_1.jpg&quot; /&gt;
We conducted Episode 1 of IEEE Alumni Series on CAN and RTOS protocols with Samuel Ramrajkar, alumni of DJ Sanghvi College and Rutgers University. He is an embedded software engineer at Aerotek and has worked in companies like Mars international and Siemens. The attendees were introduced to the concepts of real-time computing, automotive-grade electronic standards, and embedded programming fundamentals such as MISRA C.
Leveraging his experience in the industry for 5+years, he shared some of the key takeaways about the topics and how students can benefit from them.&lt;/p&gt;

&lt;h2 id=&quot;pixelated-photo-editing-workshop&quot;&gt;PIXELATED: Photo Editing Workshop:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/PIXELATED_workshop1_poster.png&quot; alt=&quot;PIXELATED 1 Poster&quot; /&gt;
&lt;img src=&quot;assets/images/Pixelated1_0.jpg&quot; alt=&quot;PIXELATED 1 Snap&quot; /&gt;
Workshop organized along with Sanjay Chauhan who is a creative editor and works as a motion graphic designer and graphic designer and is working on new projects of web development. The workshop introduced the basics of Adobe Photoshop such as spot healing, clone stamping, and CAF (Content-Aware Fill). The workshop was divided into four parts to sequentially introduce the features of the application.&lt;/p&gt;

&lt;h2 id=&quot;ieee-alumni-series-episode-2-tableau-for-advanced-analytics--business-intelligence&quot;&gt;IEEE Alumni Series Episode 2: Tableau for Advanced Analytics &amp;amp; Business Intelligence:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/Tableau_poster.png&quot; alt=&quot;Tableau&quot; /&gt;
&lt;img src=&quot;assets/images/Tableau_1.png&quot; alt=&quot;Tableau again&quot; /&gt;
IEEE in association with the IBM Data analytics program organized IEEE Alumni Series Episode 2 on Tableau for Advanced Analytics and Business Intelligence with Mr. Sachin Nagda, Quantitative Finance Analyst- Assistant Vice President at Bank of America. Mr. Sachin is an alumnus of the University of Texas and Dallas and has been working with Bank of America for 4+ years. Attendees were provided with hands-on experience on Tableau to perform data analysis and anomaly detection. It also explained how big organizations such as the Bank of America leverage Tableau to perform their strategic and analytics operations.&lt;/p&gt;

&lt;h2 id=&quot;pixelated-video-editing-workshop&quot;&gt;PIXELATED: Video Editing Workshop:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/PIXELATED_workshop2_poster.PNG&quot; alt=&quot;PIXELATED 2 Poster&quot; /&gt;
&lt;img src=&quot;assets/images/Pixelated2_0.jpg&quot; alt=&quot;PIXELATED 2 Snap&quot; /&gt;
IEEE Brainwaves in collaboration with speaker Amey Karhade organized an intercollege video editing workshop with an aim to make students familiar with Adobe After effects. Amey is a creative editor and is currently working at MONK-E as a video editor. He shares his work through his YouTube channel. The workshop introduced the concepts of video editing, swipe effect, door-effect, and clone swipe. The software used for these techniques was Adobe After Effects. A short QnA session was kept at the end.&lt;/p&gt;

&lt;h2 id=&quot;git-up&quot;&gt;Git-Up:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/Gitup_poster.jpg&quot; alt=&quot;Gitup poster&quot; /&gt;
&lt;img src=&quot;assets/images/Gitup1.png&quot; alt=&quot;Gitup 1&quot; /&gt;
&lt;img src=&quot;assets/images/Gitup2.png&quot; alt=&quot;Gitup 2&quot; /&gt;
IEEE Brainwaves in collaboration with speaker Vrushti Modi organized an event - ‚ÄúGIT UP‚Äù to emphasize open-source contributions and version control. Vrushti herself being an MLH Fellow and an open-source contributor based in Mumbai, India. It was a hands-on workshop where students learned GIT commands under the guidance of Vrushti. At the end of the session, students deployed a portfolio website on Netlify.&lt;/p&gt;

&lt;h2 id=&quot;ieee-alumni-series---episode-3---how-to-identify-high-growth-low-valuation-companies-in-equity-markets---a-masterclass-on-equity-markets-by-jay-mehta&quot;&gt;IEEE Alumni Series - Episode 3 - How to Identify High Growth Low Valuation Companies in Equity Markets - A Masterclass on Equity Markets by Jay Mehta:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/Equities_poster.jpg&quot; alt=&quot;Equities poster&quot; /&gt;
&lt;img src=&quot;assets/images/Equities_2.jfif&quot; alt=&quot;Equities 2&quot; /&gt;
IEEE in association with IBM Data Analytics Program presents How to Identify High Growth Low Valuation Companies in Equity Markets- A Masterclass on Equity Markets by Jay Mehta. The workshop provided an insight on how to get started with the stock market. The session was completely interactive, and it made sure all the questions were answered thanks to Mr. Jay Mehta and his immense knowledge about stock markets.&lt;/p&gt;

&lt;h2 id=&quot;ieee-expert-lecture-series---episode-1---fortinet-firewalls---engineering-for-an-accelerated-network-security&quot;&gt;IEEE Expert Lecture Series - Episode 1 - Fortinet Firewalls - Engineering for an Accelerated Network Security:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/Firewall_poster.jpg&quot; alt=&quot;Firewall poster&quot; /&gt;
The first episode of our newly created Lecture Series ‚Äì Fortinet Firewalls organized in collaboration with Mr. Rohan Kamat. Rohan Kamat is a Network Security Professional with 12 years of experience working with many Fortune 500 companies. Currently working as a Senior Systems Engineer, Rohan specializes in Network Security and is responsible for architecting robust solutions for customers using Fortinet technology. He gave an insight into network security and access modes, and why firewalls form an essential part of network security. Furthermore, advanced topics such as DDOS attacks were also a part of the discussion.&lt;/p&gt;

&lt;h2 id=&quot;ieee-expert-lecture-series---episode-2--technical-paper-report-writing-and-research-paper-publication&quot;&gt;IEEE Expert Lecture Series - Episode 2 ‚Äì Technical Paper, Report Writing, and Research Paper Publication:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/techpaper.png&quot; alt=&quot;IEEE Tech paper&quot; /&gt;
The second episode of our IEEE Expert Lecture series. This one delved a bit into the academic setting, the attendees were introduced to the concept of technical writing and how one can go about writing and publishing a research paper. The speakers for this event were our very own professors ‚Äì Prof. (Ms.) Darshana M. Sankhe, Prof. (Ms.) Purva Badhe, and Prof. (Mrs.) Mrunal R. Rane. The outcome of this webinar was that the students learned how to write precise and to-the-point technical papers and reports.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Experiences" />
      

      
        <summary type="html">For two years of my undergrad, I was a member of the IEEE student chapter of my institute, DJ Sanghvi College of Engineering.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">DJS Racing</title>
      <link href="/djsr" rel="alternate" type="text/html" title="DJS Racing" />
      <published>2021-07-01T03:18:00-07:00</published>
      <updated>2021-07-01T03:18:00-07:00</updated>
      <id>/djsr</id>
      <content type="html" xml:base="/djsr">&lt;p&gt;I currently lead the driverless project at DJS Racing, a student team aiming to build a self-driving Formula Student race car. In conjuction, I also developed the data acquisition system for our vehicle.&lt;/p&gt;

&lt;p&gt;Here‚Äôs a small video from 2019 about our work prior to building electric cars:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/58pZoXDAzec&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Although we‚Äôve been working remotely most of the times for the past one and half years, we‚Äôre on our way to finish building our next car by February 2022.&lt;/p&gt;

&lt;p&gt;I have always wanted to develop my own PCB in college and through the team I was able to do so. The data acquisition PCB was designed by us using the Teensy 4.1 because it offered the computation power and power efficiency that was required by our low-voltage system. The MCP2551 CAN module was used for interacing the sensors with the main board.&lt;/p&gt;

&lt;p&gt;Special thanks to SBG Systems for sponsoring us the Ellipse-N IMU, JLCPCB for manufacturing the PCBs, Analog Devices, Kvaser, and TE Connectivity for the components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/djsr_sbg.jpg&quot; alt=&quot;SBG&quot; /&gt;
&lt;img src=&quot;assets/images/djsr_kvaser.jpg&quot; alt=&quot;Kvaser&quot; /&gt;
&lt;img src=&quot;assets/images/djsr_ad.jpg&quot; alt=&quot;Analog Devices&quot; /&gt;&lt;/p&gt;

&lt;blockquote class=&quot;instagram-media&quot; data-instgrm-captioned=&quot;&quot; data-instgrm-permalink=&quot;https://www.instagram.com/p/CTKZUHqhGlh/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; data-instgrm-version=&quot;14&quot; style=&quot; background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:540px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);&quot;&gt;&lt;div style=&quot;padding:16px;&quot;&gt; &lt;a href=&quot;https://www.instagram.com/p/CTKZUHqhGlh/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; style=&quot; background:#FFFFFF; line-height:0; padding:0 0; text-align:center; text-decoration:none; width:100%;&quot; target=&quot;_blank&quot;&gt; &lt;div style=&quot; display: flex; flex-direction: row; align-items: center;&quot;&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 40px; margin-right: 14px; width: 40px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: column; flex-grow: 1; justify-content: center;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 100px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 60px;&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;padding: 19% 0;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display:block; height:50px; margin:0 auto 12px; width:50px;&quot;&gt;&lt;svg width=&quot;50px&quot; height=&quot;50px&quot; viewBox=&quot;0 0 60 60&quot; version=&quot;1.1&quot; xmlns=&quot;https://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;https://www.w3.org/1999/xlink&quot;&gt;&lt;g stroke=&quot;none&quot; stroke-width=&quot;1&quot; fill=&quot;none&quot; fill-rule=&quot;evenodd&quot;&gt;&lt;g transform=&quot;translate(-511.000000, -20.000000)&quot; fill=&quot;#000000&quot;&gt;&lt;g&gt;&lt;path d=&quot;M556.869,30.41 C554.814,30.41 553.148,32.076 553.148,34.131 C553.148,36.186 554.814,37.852 556.869,37.852 C558.924,37.852 560.59,36.186 560.59,34.131 C560.59,32.076 558.924,30.41 556.869,30.41 M541,60.657 C535.114,60.657 530.342,55.887 530.342,50 C530.342,44.114 535.114,39.342 541,39.342 C546.887,39.342 551.658,44.114 551.658,50 C551.658,55.887 546.887,60.657 541,60.657 M541,33.886 C532.1,33.886 524.886,41.1 524.886,50 C524.886,58.899 532.1,66.113 541,66.113 C549.9,66.113 557.115,58.899 557.115,50 C557.115,41.1 549.9,33.886 541,33.886 M565.378,62.101 C565.244,65.022 564.756,66.606 564.346,67.663 C563.803,69.06 563.154,70.057 562.106,71.106 C561.058,72.155 560.06,72.803 558.662,73.347 C557.607,73.757 556.021,74.244 553.102,74.378 C549.944,74.521 548.997,74.552 541,74.552 C533.003,74.552 532.056,74.521 528.898,74.378 C525.979,74.244 524.393,73.757 523.338,73.347 C521.94,72.803 520.942,72.155 519.894,71.106 C518.846,70.057 518.197,69.06 517.654,67.663 C517.244,66.606 516.755,65.022 516.623,62.101 C516.479,58.943 516.448,57.996 516.448,50 C516.448,42.003 516.479,41.056 516.623,37.899 C516.755,34.978 517.244,33.391 517.654,32.338 C518.197,30.938 518.846,29.942 519.894,28.894 C520.942,27.846 521.94,27.196 523.338,26.654 C524.393,26.244 525.979,25.756 528.898,25.623 C532.057,25.479 533.004,25.448 541,25.448 C548.997,25.448 549.943,25.479 553.102,25.623 C556.021,25.756 557.607,26.244 558.662,26.654 C560.06,27.196 561.058,27.846 562.106,28.894 C563.154,29.942 563.803,30.938 564.346,32.338 C564.756,33.391 565.244,34.978 565.378,37.899 C565.522,41.056 565.552,42.003 565.552,50 C565.552,57.996 565.522,58.943 565.378,62.101 M570.82,37.631 C570.674,34.438 570.167,32.258 569.425,30.349 C568.659,28.377 567.633,26.702 565.965,25.035 C564.297,23.368 562.623,22.342 560.652,21.575 C558.743,20.834 556.562,20.326 553.369,20.18 C550.169,20.033 549.148,20 541,20 C532.853,20 531.831,20.033 528.631,20.18 C525.438,20.326 523.257,20.834 521.349,21.575 C519.376,22.342 517.703,23.368 516.035,25.035 C514.368,26.702 513.342,28.377 512.574,30.349 C511.834,32.258 511.326,34.438 511.181,37.631 C511.035,40.831 511,41.851 511,50 C511,58.147 511.035,59.17 511.181,62.369 C511.326,65.562 511.834,67.743 512.574,69.651 C513.342,71.625 514.368,73.296 516.035,74.965 C517.703,76.634 519.376,77.658 521.349,78.425 C523.257,79.167 525.438,79.673 528.631,79.82 C531.831,79.965 532.853,80.001 541,80.001 C549.148,80.001 550.169,79.965 553.369,79.82 C556.562,79.673 558.743,79.167 560.652,78.425 C562.623,77.658 564.297,76.634 565.965,74.965 C567.633,73.296 568.659,71.625 569.425,69.651 C570.167,67.743 570.674,65.562 570.82,62.369 C570.966,59.17 571,58.147 571,50 C571,41.851 570.966,40.831 570.82,37.631&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;&lt;div style=&quot;padding-top: 8px;&quot;&gt; &lt;div style=&quot; color:#3897f0; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:550; line-height:18px;&quot;&gt;View this post on Instagram&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;padding: 12.5% 0;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: row; margin-bottom: 14px; align-items: center;&quot;&gt;&lt;div&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(0px) translateY(7px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot;background-color: #F4F4F4; height: 12.5px; transform: rotate(-45deg) translateX(3px) translateY(1px); width: 12.5px; flex-grow: 0; margin-right: 14px; margin-left: 2px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(9px) translateY(-18px);&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;margin-left: 8px;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 20px; width: 20px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; width: 0; height: 0; border-top: 2px solid transparent; border-left: 6px solid #f4f4f4; border-bottom: 2px solid transparent; transform: translateX(16px) translateY(-4px) rotate(30deg)&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;margin-left: auto;&quot;&gt; &lt;div style=&quot; width: 0px; border-top: 8px solid #F4F4F4; border-right: 8px solid transparent; transform: translateY(16px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; flex-grow: 0; height: 12px; width: 16px; transform: translateY(-4px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot; width: 0; height: 0; border-top: 8px solid #F4F4F4; border-left: 8px solid transparent; transform: translateY(-4px) translateX(8px);&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: column; flex-grow: 1; justify-content: center; margin-bottom: 24px;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 224px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 144px;&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;p style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;&quot;&gt;&lt;a href=&quot;https://www.instagram.com/p/CTKZUHqhGlh/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;&quot; target=&quot;_blank&quot;&gt;A post shared by DJS Racing (@djs_racing)&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//www.instagram.com/embed.js&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;djs-racing-driverless&quot;&gt;DJS Racing Driverless&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/djsre03.jpg&quot; alt=&quot;E03&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The autonomous systems team of DJS Racing was formed back in early 2019 to develop a prototype vehicle. Alongside, we have also been developing the software that goes along with it. The team will use the software system presented below for the Formula Student Driverless competition in 2022 (postponed due to the outbreak of COVID-19).&lt;/p&gt;

&lt;h2 id=&quot;about-formula-student-driverless&quot;&gt;About Formula Student Driverless&lt;/h2&gt;

&lt;p&gt;Formula Student Driverless (FSD) is the world‚Äôs largest autonomous racing competition, held annually in Hockenheim, Germany, in which multidisciplinary student teams compete with self-developed driverless racecars every year. In this competition, each team has to race their autonomous car across 5 disciplines - Acceleration, Skidpad, Autocross, and Trackdrive. In the main event, called Trackdrive, a car has to complete 10 laps on an unknown track autonomously as fast as possible against the clock. The track is defined by a set of traffic cones, blue cones on the left and yellow cones on the right-hand side&lt;a href=&quot;#27&quot;&gt;[27]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;track-layout&quot;&gt;Track Layout&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/track_layout.jpg&quot; alt=&quot;Track Layout&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this blog, I shall be describing the overall implementation of a Formula Student Driverless race-car by DJS Racing, India. The race-car can drive autonomously on unknown tracks and switch to Model Predictive Control after track exploration, mapping, and loop-closure detection in the first lap.&lt;/p&gt;

&lt;h2 id=&quot;software-architecture&quot;&gt;Software Architecture&lt;/h2&gt;

&lt;p&gt;We chose the Robot Operating System as our primary framework for software development. In order to run the MPC algorithm at its full potential, the track must be known at least 2s in advance for a speed of 85 kmph. With the current perception setup, it is not possible to achieve accurate mapping of the environment with a 40m (at 85 kmph) of look-ahead. Hence, we need to drive the first lap, exploring the track using a geometric controller such as the Pure Pursuit Controller&lt;a href=&quot;#23&quot;&gt;[23]&lt;/a&gt;&lt;a href=&quot;#28&quot;&gt;[28]&lt;/a&gt; at a lower speed. After successful mapping and exploration of the track, the car switches to the more performant Model Predictve Control(MPC) for the subsequent laps to race as fast as possible. From this, we can define two modes of our system operation, SLAM mode, and localization mode. To count the number of laps and to detect loop closure, high-level mission planning is done using a finite state machine. This state machine guides to the system to switch between SLAM and localization mode and signals whether the race has finished. Interfacing between the onboard Jetson AGX Xavier and the STM32 is done using the ros-serial package. The STM32 runs FreeRTOS to achieve real-time control of actuators.&lt;/p&gt;

&lt;h4 id=&quot;system-overview&quot;&gt;System Overview&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/Autonomous System.png&quot; alt=&quot;AS&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;high-level-architecture&quot;&gt;High-level Architecture&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/rosgraph.png&quot; alt=&quot;ROS Graph&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3d-perception&quot;&gt;3D Perception&lt;/h2&gt;

&lt;p&gt;The goal of the perception system is to detect traffic cones in the vicinity of the car and determine their color and 3D coordinates in real-time. These cone detections will be then fed as landmarks to the SLAM node to build a map of the track. The perception system takes input from multiple sensors - stereo camera, and a 3D LiDAR. The Basler Ace 2 industrial camera was chosen for our self-developed mono and stereo camera setups.&lt;/p&gt;

&lt;p&gt;Multiple sources of visual information are chosen so as to increase the redundancy&lt;a href=&quot;#22&quot;&gt;[22]&lt;/a&gt; in the system and to perceive the environment reliably as only one chance is given for the trackdrive race.&lt;/p&gt;

&lt;h4 id=&quot;lidar-3d-object-dectection&quot;&gt;LiDAR 3D Object Dectection&lt;/h4&gt;

&lt;p&gt;3D Traffic Cone Detection using LiDAR Data&lt;/p&gt;

&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
	&lt;source src=&quot;assets/videos/lidar.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;The point cloud obtained from the LiDAR is first segmented and the ground and out-of-track planes are removed. Afterwards, Euclidean clustering is run on the point cloud to cluster cone patches. The centroid of these cone patches is used to determine the 3D coordinates for each cone.&lt;/p&gt;

&lt;p&gt;These clusters are also passed through a rule-based outlier filter to remove false cones in the map.&lt;/p&gt;

&lt;p&gt;We don‚Äôt estimate the color for each detected cone since the computational costs don‚Äôt justify its use for this year and the path planning algorithm works just as well without color information.&lt;/p&gt;

&lt;h4 id=&quot;stereo-3d-object-dectection&quot;&gt;Stereo 3D Object Dectection&lt;/h4&gt;

&lt;p&gt;In order to ensure redundancy in the system, an additional camera-based object detection pipeline is added to the system.&lt;/p&gt;

&lt;p&gt;We have chosen an off-the-shelf object detector, YOLOv3&lt;a href=&quot;#17&quot;&gt;[17]&lt;/a&gt; for object detection for real-time performance.&lt;/p&gt;

&lt;p&gt;Stereo Object Detection Pipeline&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/stereo_pipeline.png&quot; alt=&quot;Stereo Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;YOLOv3 running on mock cones&lt;/p&gt;

&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
	&lt;source src=&quot;assets/videos/stereo.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Stereo feature matching is then performed using the Semi-Global Block Matching&lt;a href=&quot;#32&quot;&gt;[32]&lt;/a&gt; (SGBM) algorithm to obtain the disparity map and 3D coordinates of the traffic cones.&lt;/p&gt;

&lt;p&gt;3D point cloud obtained after occlusion reduction in the disparity map&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/stereo_depth.jpg&quot; alt=&quot;Stereo Depth&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Occlusion of background cones is handled by outlier filtering.&lt;/p&gt;

&lt;h2 id=&quot;velocity-estimation-and-simultaneous-localization-and-mapping&quot;&gt;Velocity Estimation and Simultaneous Localization and Mapping&lt;/h2&gt;

&lt;p&gt;The extended Kalman Filter was chosen as an estimator for mildly non-linear systems with white Gaussian noise. Our main task here is to fuse data from an Inertial Measurement Unit (SBG Ellipse N), 4 wheel encoders, steering angle sensor, and a dual GPS system in a moving baseline configuration. EKF is also computationally more efficient than the Unscented Kalman Filter&lt;a href=&quot;#20&quot;&gt;[20]&lt;/a&gt; and allows us to estimate the state at 200 Hz. Here, the effects of lateral load transfer are ignored and the track is assumed to be a flat 2D surface.&lt;/p&gt;

&lt;p&gt;State Estimation Pipeline&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/velocity_estimation.png&quot; alt=&quot;Velocity Estimation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We consider that our system model is a constant velocity model (CTRV)&lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;, the jerk is assumed to be zero in this case and acceleration is used as a pseudo input to the system.&lt;/p&gt;

&lt;p&gt;The wheel encoders along with the steering angle potentiometer are used to estimate the slip ratios of the vehicle.&lt;/p&gt;

&lt;p&gt;The 6-DOF state vector is taken as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/state_vector.jpg&quot; alt=&quot;State Vector&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p, Œ∏, v&lt;/code&gt; are the position (along x and y), angular velocity, and the linear velocity (along x and y in car frame) of the car respectively.&lt;/p&gt;

&lt;p&gt;The process model is taken as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/process_model.jpg&quot; alt=&quot;Process model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; is the linear acceleration measured by the IMU and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R(Œ∏)&lt;/code&gt; is the rotation matrix between the sensor frame and the vehicle frame.&lt;/p&gt;

&lt;p&gt;The sensor model is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/sensor_model.jpg&quot; alt=&quot;Sensor model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Œ∏s&lt;/code&gt; is the sensor heading in the body frame and n{.} are additive Gaussian white noises that corrupt the measurements.&lt;/p&gt;

&lt;p&gt;Different sensors work at different rates and the accuracy also varies between the sensors. Hence, we use multiple update functions for each sensor and we update our beliefs asynchronously&lt;a href=&quot;#26&quot;&gt;[26]&lt;/a&gt;. Here, the acceleration is taken from the IMU, angular velocity from the Gyroscope, and velocities from GPS and wheel encoders.&lt;/p&gt;

&lt;p&gt;The landmark-based FastSLAM&lt;a href=&quot;#8&quot;&gt;[8]&lt;/a&gt;&lt;a href=&quot;#15&quot;&gt;[15]&lt;/a&gt; algorithm was chosen for its robustness because of multiple-hypothesis and proposal sampling&lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt;&lt;a href=&quot;#31&quot;&gt;[31]&lt;/a&gt;. FastSLAM is computationally much more efficient with N.log(N) time complexity as compared to EKF SLAM with a quadratic complexity &lt;a href=&quot;#19&quot;&gt;[19]&lt;/a&gt;. FastSLAM is also much more tunable owing to the fact that it uses a Rao-Backwellized particle filter to express pose estimates. We can simply tune the number of particles to adjust the runtime performance. Data association is done by comparing the mahalanobis distance between an observation and each landmark on the map.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/slam_pipeline.jpg&quot; alt=&quot;SLAM pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our SLAM pipeline runs at 15 Hz to accommodate for the perception pipeline operation rate. The pose estimates from the SLAM node are then integrated by the velocity with time by the velocity estimation node to provide fast pose updates at 200 Hz for the control algorithm. This ensures that the MPC node always receives the latest pose estimates.&lt;/p&gt;

&lt;h2 id=&quot;path-planning&quot;&gt;Path Planning&lt;/h2&gt;

&lt;h4 id=&quot;rrt--delaunay-triangulation-for-waypoints-generation&quot;&gt;RRT + Delaunay Triangulation for Waypoints Generation&lt;/h4&gt;

&lt;p&gt;We chose the Randomly Exploring Random Trees&lt;a href=&quot;#24&quot;&gt;[24]&lt;/a&gt;&lt;a href=&quot;#25&quot;&gt;[25]&lt;/a&gt; as our path planning algorithms. Formula Student poses a unique challenge to path planning as there is no definite goal, and we just have to drive straight. Hence, in order to determine the best path, we have proposed a cost function that is assigned to each node. The tree branch with the lowest cost is then chosen as our desired path.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;J = Q_delta * delta^2 + Q_theta * theta^2 + Q_cte * cte^2 - Q_cone * cone_count^2&lt;/code&gt;&lt;br /&gt;
Where,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;delta: Angular deviation between two nodes. Avoids sharp turns.&lt;/li&gt;
  &lt;li&gt;theta: Discounted heading. Tree pointing backward to the car are penalized.&lt;/li&gt;
  &lt;li&gt;cte: Distance between the path and track boundary. Penalizes branches too close to the boundaries.&lt;/li&gt;
  &lt;li&gt;cone_count: Count of cones on both sides of the path. Penalizes shorter paths.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The cost is then normalized between 0 and 1 to determine the best path.&lt;/p&gt;

&lt;p&gt;However, this is not the appropriate approach to path planning in FSD, as we need to have a continous and smooth trajectory in order to ensure that we are driving close to the limits of handling. Hence, we have an additional step of running Delaunay triangulation&lt;a href=&quot;#13&quot;&gt;[13]&lt;/a&gt; on nearby cone positions. The intersection points of the best tree branch and the traingulation segments are then taken as the waypoints for the control algorithm.&lt;/p&gt;

&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
	&lt;source src=&quot;assets/videos/rrt_delaunay.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;The waypoints that are generated are depicted as blue dots along the track center-line.&lt;/p&gt;

&lt;h2 id=&quot;model-predictive-control&quot;&gt;Model Predictive Control&lt;/h2&gt;

&lt;p&gt;After finishing the first lap using the PID and Pure Pursuit Controller, loop closure is detected and the SLAM algorithm switches to localization mode. This is all handled by the mission planning node. After track exploration and mapping, we switch to Model Predictve Control&lt;a href=&quot;#18&quot;&gt;[18]&lt;/a&gt; as our control method to finish the race as fast as possible at the limits of handling in real-time.&lt;/p&gt;

&lt;p&gt;The goal of the optimization problem is to drive the car along the reference path, following imposed contraints and targeting the desired velocity. The state dynamics is considered as a bicycle model with actuator and drivetrain models along with tire forces.&lt;/p&gt;

&lt;h4 id=&quot;dynamic-bicycle-model&quot;&gt;Dynamic Bicycle Model&lt;/h4&gt;

&lt;p&gt;We consider the dynamic bicycle model for vehicle dynamics&lt;a href=&quot;#9&quot;&gt;[9]&lt;/a&gt;&lt;a href=&quot;#14&quot;&gt;[14]&lt;/a&gt; with non-linear magic tire formula models&lt;a href=&quot;#16&quot;&gt;[16]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/dynamic_model.jpg&quot; alt=&quot;Dynamic model&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;vehicle-dynamics&quot;&gt;Vehicle Dynamics&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/vehicle_dynamics.jpg&quot; alt=&quot;Vehicle dynamics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;em&gt;X, Y&lt;/em&gt; is the position and is the orientation of the car in the world frame. &lt;em&gt;Vx, Vy&lt;/em&gt;, and &lt;em&gt;œâ&lt;/em&gt; are the linear and angular velocities of the car in the body frame respectively. &lt;em&gt;ùõø&lt;/em&gt; is the steering angle, &lt;em&gt;Lf&lt;/em&gt; and &lt;em&gt;Lr&lt;/em&gt; are the length from the front and rear axles to the center-of-gravity respectively and &lt;em&gt;Iz&lt;/em&gt; is the inertia moment.&lt;/p&gt;

&lt;h4 id=&quot;tire-model&quot;&gt;Tire Model&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/tire_model.jpg&quot; alt=&quot;Tire model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/tire_model_x.jpg&quot; alt=&quot;Tire model x&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here &lt;em&gt;D, C, B&lt;/em&gt; are coefficients obtained from the Pacejka tire model. &lt;em&gt;d&lt;/em&gt; is the driving command (acceleration), &lt;em&gt;Cr&lt;/em&gt; is the rolling resistance, &lt;em&gt;Œ±{f, r}&lt;/em&gt; are tire slip angles, and &lt;em&gt;Cd&lt;/em&gt; is the drag coefficient. &lt;em&gt;m&lt;/em&gt; is the mass of the vehicle.&lt;/p&gt;

&lt;h4 id=&quot;state-input&quot;&gt;State Input&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/state_input.jpg&quot; alt=&quot;State input&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;the-lateral-and-cross-track-errors-are-defined-as&quot;&gt;The lateral and cross-track errors are defined as&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cte` = cte - v * sin(epsi) * dt&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epsi` = epsi + v / Lf * (-delta) * dt&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;and-hence-the-cost-function-is-defined-as&quot;&gt;And hence, the cost function is defined as&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;J = Q_cte * cte^2 + Q_epsi * epsi^2 + Q_v * (v - vmax)^2 + Q_delta * delta^2&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+ Q_a * a^2 + F * (a` - a)^2 + Q_ddelta * (delta` - delta)^2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The objective of this optimization problem is to drive the vehicle as close as possible to the track center line, with heading along the track direction. Rapid changes in the state inputs, as well as linear and angular velocities, are penalized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/videos/mpc.gif&quot; alt=&quot;this slowpoke moves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimization problem is then solved directly using the IPOPT non-linear optimization library&lt;a href=&quot;#6&quot;&gt;[6]&lt;/a&gt; in a receding horizon fashion for over 15 steps for a horizon of 1.5 secs. There is an approximately 100 ms delay between computed command and actuation, hence we choose an actuator vector in the future (about 100 ms in the future) to compensate for mechanical delays.&lt;br /&gt;
However, an improvement is required here as IPOPT is not designed for real-time systems and considerably slows down with an increase in sequence size&lt;a href=&quot;#30&quot;&gt;[30]&lt;/a&gt;. Hence, we need to use a convex optimization library such as HPIPM&lt;a href=&quot;#29&quot;&gt;[29]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;simulation-and-cicd-infrastructure&quot;&gt;Simulation and CI/CD Infrastructure&lt;/h2&gt;

&lt;h4 id=&quot;simulation-environment-designed-with-gazebo12-and-ignition-libraries&quot;&gt;Simulation Environment Designed with Gazebo&lt;a href=&quot;#12&quot;&gt;[12]&lt;/a&gt; and Ignition Libraries&lt;/h4&gt;

&lt;p&gt;The FSSIM &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt; simulation framework by AMZ Driverless has been used for the codebase. The FSSIM simulator uses a Gazebo plugin that uses a basic vehicle model which is discretized with Euler Forward discretization to achieve 99% of real-world performance.&lt;/p&gt;

&lt;h4 id=&quot;ros-bags-from-github-pull-requests-and-actual-car-races-can-be-visualized-in-the-app&quot;&gt;ROS Bags from GitHub Pull Requests and actual car races can be visualized in the app&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/image3.jpg&quot; alt=&quot;ROS Bag Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simulation framework, on every pull request, runs the codebase and important ROS topics are recorded into ROS Bags and uploaded to the server. Each ROS Bag is played on the remote server and the run is recorded using VirtualGL, the videos are then stored in a NoSQL database for backup and querying. The client app can establish a remote database connection to visualize the ROS Bags. The server automatically replays the bag file and stores the recorded videos. To ensure software reliability, each pull request is first compiled using GitHub Actions and run on our CI server hosted on AWS. Only after all checks have passed and the simulation runs without any issues, a pull request can be merged.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Kieran Strobel and Sibo Zhu and Raphael Chang and Skanda Koppula. Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions. 2020. arXiv:2007.13971 &lt;a id=&quot;1&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;de la Iglesia Valls, M., Hendrikx, H. F. C., Reijgwart, V. J. F., Meier, F. V., Sa, I., Dub¬¥e, R., Gawel, A., B¬®urki, M., and Siegwart, R. (2018). Design of an autonomous racecar: Perception, state estimation and system integration. 2018 IEEE International Conference on Robotics and Automation (ICRA). &lt;a id=&quot;2&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dellaert, F., Fox, D., Burgard, W., and Thrun, S. (1999). Monte carlo localization for mobile robots. In IEEE International Conference on Robotics and Automation (ICRA99). &lt;a id=&quot;3&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Juraj Kabzan and Miguel de la Iglesia Valls and Victor Reijgwart and Hubertus Franciscus Cornelis Hendrikx and Claas Ehmke and Manish Prajapat and Andreas B√ºhler and Nikhil Gosala and Mehak Gupta and Ramya Sivanesan and Ankit Dhall and Eugenio Chisari and Napat Karnchanachari and Sonja Brits and Manuel Dangel and Inkyu Sa and Renaud Dub√© and Abel Gawel and Mark Pfeiffer and Alexander Liniger and John Lygeros and Roland Siegwart. AMZ Driverless: The Full Autonomous Racing System. 2019. arXiv:1905.05150. &lt;a id=&quot;4&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dhall, A., Dai, D., and Van Gool, L. (2019). Real-time 3D Traffic Cone Detection for Autonomous Driving. arXiv e-prints, page arXiv:1902.02394. &lt;a id=&quot;5&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;W√§chter, A., Biegler, L. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Math. Program. 106, 25‚Äì57 (2006). https://doi.org/10.1007/s10107-004-0559-y. &lt;a id=&quot;6&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;B. D. Brumback and M. Srinath, ‚ÄúA chi-square test for fault-detection in kalman filters,‚Äù Automatic Control, IEEE Transactions on, vol. 32, pp. 552 ‚Äì 554, 07 1987. &lt;a id=&quot;7&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;M. Montemerlo, S. Thrun, D. Koller, B. Wegbreit, et al., ‚ÄúFastslam: A factored solution to the simultaneous localization and mapping problem,‚Äù in Aaai/iaai, pp. 593‚Äì598, 2002. &lt;a id=&quot;8&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kong, Jason, et al. ‚ÄúKinematic and dynamic vehicle models for autonomous driving control design.‚Äù 2015 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2015. &lt;a id=&quot;9&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yoon, Jong-Hwa, and Huei Peng. ‚ÄúA cost-effective sideslip estimation method using velocity measurements from two GPS receivers.‚Äù IEEE Transactions on Vehicular Technology 63.6 (2013): 2589-2599. &lt;a id=&quot;10&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Klomp, M., Olsson, K., and Sandberg, C. (2014). Non-linear steering control for limit handling conditions using preview path curvature. International Journal of Vehicle Autonomous Systems, 12(3):266‚Äì283. &lt;a id=&quot;11&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Koenig, N. and Howard, A. (2004). Design and use paradigms for gazebo, an open-source multi-robot simulator. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, volume 3, pages 2149‚Äì2154. IEEE. &lt;a id=&quot;12&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lee and Schachter (1980). Two algorithms for constructing a delaunay triangulation. International Journal of Computer and Information Sciences. &lt;a id=&quot;13&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Milliken, W., Milliken, D., and of Automotive Engineers, S. (1995). Race Car Vehicle Dynamics. Premiere Series. SAE International. &lt;a id=&quot;14&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Montemerlo, M., Thrun, S., Koller, D., and Wegbreit, B. (2003). FastSLAM 2.0 : An Improved Particle Filtering Algorithm for Simultaneous Localization and Mapping that Provably Converges. In Proceedings of the 18th international joint conference on Artificial intelligence, pages 1151‚Äì1156. &lt;a id=&quot;15&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pacejka, H. B. and Bakker, E. (1992). The magic formula tyre model. Vehicle system dynamics, 21(S1):1‚Äì18. &lt;a id=&quot;16&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You only look once: Unified, real-time object detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 779‚Äì788. &lt;a id=&quot;17&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rosolia, U., Carvalho, A., and Borrelli, F. (2017). Autonomous racing using learning model predictive control. In American Control Conference (ACC), pages 5115‚Äì5120. IEEE. &lt;a id=&quot;18&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Thrun, S., Burgard, W., and Fox, D. (2005). Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press. &lt;a id=&quot;19&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Xue, Z. and Schwartz, H. (2013). A comparison of several nonlinear filters for mobile robot pose estimation. In 2013 IEEE International Conference on Mechatronics and Automation. &lt;a id=&quot;20&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tulsiani, S. and Malik, J. (2015). Viewpoints and keypoints. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). &lt;a id=&quot;21&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gosala, N. B., B¬®uhler, A., Prajapat, M., Ehmke, C., Gupta, M., Sivanesan, R., Gawel, A., Pfeiffer, M., B¬®urki, M., Sa, I., Dub¬¥e, R., and Siegwart, R. (2018). Redundant Perception and State Estimation for Reliable Autonomous Racing. ArXiv e-prints. &lt;a id=&quot;22&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Coulter, R. C. (1992). Implementation of the pure pursuit path tracking algorithm. Technical report, Carnegie-Mellon UNIV Pittsburgh PA Robotics INST. &lt;a id=&quot;23&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LaValle, Steven M. (October 1998). ‚ÄúRapidly-exploring random trees: A new tool for path planning‚Äù (PDF). Technical Report. Computer Science Department, Iowa State University (TR 98‚Äì11). &lt;a id=&quot;24&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Howie Choset, James Kuffner. ‚ÄúRobotic Motion Planning: RRT‚Äôs‚Äù. Robotic Motion Planning, 16-735. The Robotics Institute, Carnegie Mellon University. &lt;a id=&quot;25&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;K. Hausman, S. Weiss, R. Brockers, L. Matthies, and G. S. Sukhatme, ‚ÄúSelf-calibrating multi-sensor fusion with probabilistic measurement validation for seamless sensor switching on a uav,‚Äù in 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 4289‚Äì 4296, May 2016. &lt;a id=&quot;26&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Formula Student Rules 2020 v1.0. https://www.formulastudent.de/fileadmin/user_upload/all/2020/rules/FS-Rules_2020_V1.0.pdf &lt;a id=&quot;27&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Ni and J. Hu, ‚ÄúPath following control for autonomous formula racecar: Autonomous formula student competition,‚Äù 2017 IEEE Intelligent Vehicles Symposium (IV), Los Angeles, CA, 2017, pp. 1835-1840, doi: 10.1109/IVS.2017.7995972. &lt;a id=&quot;28&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;G. Frison, M. Diehl. HPIPM: a high-performance quadratic programming framework for model predictive control. 2020. arXiv preprint, arXiv:2003.02547 &lt;a id=&quot;29&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;G. Frison, D. K. M. Kufoalor, L. Imsland and J. B. J√∏rgensen, ‚ÄúEfficient implementation of solvers for linear model predictive control on embedded devices,‚Äù 2014 IEEE Conference on Control Applications (CCA), Juan Les Antibes, 2014, pp. 1954-1959, doi: 10.1109/CCA.2014.6981589. &lt;a id=&quot;30&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. D. Hol, T. B. Schon, and F. Gustafsson, ‚ÄúOn resampling algorithms for particle filters,‚Äù in Nonlinear Statistical Signal Processing Workshop, 2006 IEEE, pp. 79‚Äì82, IEEE, 2006. &lt;a id=&quot;31&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SGBM, by Hirschmuller, H. (2008). Stereo processing by semiglobal matching and mutual information. IEEE Transactions on pattern analysis and machine intelligence, 30(2), 328-341. &lt;a id=&quot;32&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      
        <category term="Experiences" />
      

      
        <summary type="html">I currently lead the driverless project at DJS Racing, a student team aiming to build a self-driving Formula Student race car. In conjuction, I also developed the data acquisition system for our vehicle.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Internship at Mowito Robotics</title>
      <link href="/mowito" rel="alternate" type="text/html" title="Internship at Mowito Robotics" />
      <published>2021-05-30T03:18:00-07:00</published>
      <updated>2021-05-30T03:18:00-07:00</updated>
      <id>/mowito</id>
      <content type="html" xml:base="/mowito">&lt;p&gt;Mowito is an Indian startup that provides autonomous navigation software solutions for use in warehouses and more. The company has also recently expanded into providing physical robots with applications such as grocery delivery.&lt;/p&gt;

&lt;p&gt;Mowito being a startup, provided me a lot of avenues to learn new things. The work ranged from robot navigation, lidar stitching to cloud computing. Often the work involved projects that either the team or nobody in the entire world has experience working. This helped me quickly adapt to situations and learn new things in a short time, often required for an early-stage startup. I had great mentors who bestowed me with the responsibility to directly work on their navigation stack - their core offering that is used in warehouse environments by companies around the world. The stack was being directly deployed on ROS-based robots that were being remotely monitored by us. Under the mentor‚Äôs guidance, I was able to learn agile project management, startup accelerators, how to manage people in a company.&lt;/p&gt;

&lt;p&gt;A short video showing some of the deployments using Mowito‚Äôs navigation stack:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/X_pU2qYaPRc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In this blog, I will be presenting the work done by me during the internship.&lt;/p&gt;

&lt;h2 id=&quot;human-feet-detection&quot;&gt;Human feet detection&lt;/h2&gt;

&lt;p&gt;We ported the ROS leg-detector package to support ROS 2 while making it more reliable and accurate. The package was required by one of our clients to detect workers in their warehouses. The load-carrying robot changes its global trajectory if it encounters a dynamic obstacle.&lt;/p&gt;

&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot;&gt;
  &lt;source src=&quot;assets/videos/foot_detector.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/mw_leg_detector.png&quot; alt=&quot;Leg Detector Screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the above demo, we used an RPLIDAR, with a resolution of 0.33 degrees at 10 Hz. The lidar map was converted to an image array to detect human foot clusters. After data association with existing foot position priors, the extended Kalman filter was updated from the measurements.&lt;/p&gt;

&lt;p&gt;Oh, by the way, the ROS Discourse post was featured in ROS News for the Week of January 10th, 2021 - &lt;a href=&quot;https://discourse.ros.org/t/ros-news-for-the-week-of-january-10th-2021/18488&quot;&gt;https://discourse.ros.org/t/ros-news-for-the-week-of-january-10th-2021/18488&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;motion-primitives-generation&quot;&gt;Motion Primitives Generation&lt;/h2&gt;

&lt;p&gt;The maxl controller developed by Mowito uses motion primitives to generate local paths for navigation. The motion primitives are computed using the robot dimensions, path fan angle, expand distance, and more.&lt;/p&gt;

&lt;p&gt;Here‚Äôs a GIF showing possible paths for a robot using motion primitives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/mw_maxl_demo.gif&quot; alt=&quot;MAXL Demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The primitives are generated using a nearest-neighbor query approach using the SciPy‚Äôs KDTree module to achieve linear-logarithmic time complexity. The KD tree is used to find the nearest data points in n-dimensional NumPy arrays. Further, using basic trigonometry, the paths are generated.&lt;/p&gt;

&lt;p&gt;Link to the tool - &lt;a href=&quot;http://ec2-15-207-68-243.ap-south-1.compute.amazonaws.com/&quot;&gt;http://ec2-15-207-68-243.ap-south-1.compute.amazonaws.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here‚Äôs how the primitives look like:
&lt;img src=&quot;assets/images/mw_mprim_plot.png&quot; alt=&quot;assets/images/mw_mprim_plot.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;improvements-to-maxl-controller&quot;&gt;Improvements to MaxL controller&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/mw_maxl_rosbot.png&quot; alt=&quot;assets/images/mw_maxl_rosbot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also worked on the core MaxL navigation stack and the controller. My initial work was related to improving the function stubs to get myself more familiar with the codebase. After that my contributions were to reduce the number of oscillations in the planned path, architecture improvements, automated testing, and benchmarking. The controller previously had some code duplication between ROS 1 and 2, I decoupled the core library (the pure pursuit controller and some other stuff) and made it into a single CMake package which is now used by the ROS 1 and 2 packages as a CMake subdirectory. This greatly saved development time, and a developer needed to only make changes to one repository to improve both ROS 1 and 2 implementations.&lt;/p&gt;

&lt;p&gt;Some other improvements included grouping similar function parameters into a single struct to reduce the number of parameters being passed into the functions. Some functions previously had as many as 60 parameters for path planning. Consequently, they hover around 12 now (Structs hold the individual values).&lt;/p&gt;

&lt;h2 id=&quot;sensor-calibration-and-fusion&quot;&gt;Sensor calibration and fusion&lt;/h2&gt;

&lt;p&gt;I had the opportunity to also work on the client-facing services at Mowito. Some packages developed by me served to calibrate wheel encoders for differential drive robots and fusing point clouds from multiple lidars.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;LiDAR fusion:&lt;/b&gt;
A ROS 2 package for fusing data provided by multiple lidars placed on a robot.&lt;/p&gt;

    &lt;p&gt;GitHub repository: &lt;a href=&quot;https://github.com/mowito/mw_lidar_fusion&quot;&gt;https://github.com/mowito/mw_lidar_fusion&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;The point clouds are fused using the TF transform between two link frames, and matching ROS messages by the time.&lt;/p&gt;

    &lt;p&gt;Here‚Äôs a demo showing two fused point clouds with white points depicting data points that fall within a certain angle after filtering:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;assets/images/mw_lidar_fusion.png&quot; alt=&quot;assets/images/mw_lidar_fusion.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Wheel encoder calibration:&lt;/b&gt;
Our deployments often needed periodic manual maintenance to function well. One recurring problem was a deviation in the pose estimated using wheel odometry. Due to wheel slip and other factors, a bias was introduced in the system that led to the robot going way off track from its goal position. This was a critical issue since these robots were handling fragile pieces of equipment in the warehouses.&lt;/p&gt;

    &lt;p&gt;GitHub repository: &lt;a href=&quot;https://github.com/mowito/odometry_calibration&quot;&gt;https://github.com/mowito/odometry_calibration&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;Here are the plots for reference count per second vs wheel encoder feedback value for both left and right wheels:
&lt;img src=&quot;assets/images/mw_calib_left.png&quot; alt=&quot;assets/images/mw_calib_left.png&quot; /&gt;
&lt;img src=&quot;assets/images/mw_calib_right.png&quot; alt=&quot;assets/images/mw_calib_right.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;The wheel encoders didn‚Äôt perform well whenever a sudden change in motion was observed. A correction factor is thus introduced while calculating the robot angular velocity or the yaw rate.&lt;/p&gt;

    &lt;p&gt;Odometry plot after yaw correction:
&lt;img src=&quot;assets/images/mw_odom_yaw.png&quot; alt=&quot;assets/images/mw_odom_yaw.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;maxl-vs-nav2-dwb&quot;&gt;MaxL vs Nav2 DWB&lt;/h2&gt;

&lt;p&gt;If you have read so far, then you might probably wonder - why use a proprietary controller instead of an already widely used one such as DWA/DWB offered by ROS Navigation?&lt;/p&gt;

&lt;p&gt;Well, the answer is that in some cases you might want to leverage the reduced computational cost of running the MaxL controller - it runs without using a cost map! Another benefit is that in most cases, MaxL provides more smooth paths and fewer recoveries during the robot‚Äôs operation.&lt;/p&gt;

&lt;p&gt;Here‚Äôs a comparison between MaxL and ROS Nav2‚Äôs DWB over their execution times:
&lt;img src=&quot;assets/images/mw_maxl_vs_dwb.png&quot; alt=&quot;assets/images/mw_maxl_vs_dwb.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In most cases, MaxL is faster than the DWB controller albeit with a higher variance. It is also important to notice that the DWB controller gets much slower as the cost map resolution increases.&lt;/p&gt;

&lt;p&gt;During the internship, I created an automated testing tool to benchmark the two controllers. The CGAL library was used to spawn random objects with a predefined area limit, to stress the two controllers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/mw_maxl_ats.png&quot; alt=&quot;assets/images/mw_maxl_ats.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above is a screenshot of an RViz window depicting a 5 x 5 room with 40% of its area being occupied. The lidar points are generated circularly with a ray being cast onto the object at each iteration. The intersection point is then computed using CGAL and added to the lidar point cloud array. The demo tries to replicate an RPLiDAR with a 0.33 degrees resolution operating at 10 Hz.&lt;/p&gt;

&lt;p&gt;Thank you for reading.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Experiences" />
      

      
        <summary type="html">Mowito is an Indian startup that provides autonomous navigation software solutions for use in warehouses and more. The company has also recently expanded into providing physical robots with applications such as grocery delivery.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">COVID-19 Detection from Chest X-Ray Images</title>
      <link href="/resnet-chest" rel="alternate" type="text/html" title="COVID-19 Detection from Chest X-Ray Images" />
      <published>2020-12-20T02:18:00-08:00</published>
      <updated>2020-12-20T02:18:00-08:00</updated>
      <id>/resnet-chest</id>
      <content type="html" xml:base="/resnet-chest">&lt;p&gt;A small project that I did back in December of 2020 for COVID-19 detection using chest x-ray images. The model uses a modified ResNet-50 model for detection using RGB signals.&lt;/p&gt;

&lt;p&gt;GradCAM activation map using the ResNet-50 model:
&lt;img src=&quot;assets/images/cd-blog.png&quot; alt=&quot;GradCAM for activation map visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CLAHE (Contrast Limited Adaptive Histogram Equalization) is also applied to highlight key areas in the scanned images:
&lt;img src=&quot;assets/images/cd-clahe.png&quot; alt=&quot;CLAHE for COVID detection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For complete implementation, visit &lt;a href=&quot;https://colab.research.google.com/drive/1WSC3at-tNeRX-phdlGnnJYiagJwElp5o?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1WSC3at-tNeRX-phdlGnnJYiagJwElp5o?usp=sharing&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Experiences" />
      

      
        <summary type="html">A small project that I did back in December of 2020 for COVID-19 detection using chest x-ray images. The model uses a modified ResNet-50 model for detection using RGB signals.</summary>
      

      
      
    </entry>
  
</feed>
