<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/projects/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-09-19T19:28:47+05:30</updated>
  <id>http://localhost:4000/tag/projects/feed.xml</id>

  
  
  

  
    <title type="html">Atharva Pusalkar | </title>
  

  
    <subtitle>Atharva Pusalkar</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Contributing to Open Robotics</title>
      <link href="http://localhost:4000/osrf" rel="alternate" type="text/html" title="Contributing to Open Robotics" />
      <published>2021-08-23T15:48:00+05:30</published>
      <updated>2021-08-23T15:48:00+05:30</updated>
      <id>http://localhost:4000/osrf</id>
      <content type="html" xml:base="http://localhost:4000/osrf">&lt;p&gt;In this post I will be presenting my contributions to Open Robotics before joining them as a student developer. I contributed mainly to the Ignition Gazebo and Ignition RViz projects for a few months to get familiar with a large codebase.&lt;/p&gt;

&lt;h2 id=&quot;ignition-gazebo&quot;&gt;Ignition Gazebo&lt;/h2&gt;

&lt;h3 id=&quot;about-dialog&quot;&gt;About dialog&lt;/h3&gt;

&lt;p&gt;Adds an about dialog box to display version number, license, and documentation links.&lt;/p&gt;

&lt;p&gt;Pull request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/609&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/609&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/about_dialog.png&quot; alt=&quot;About dialog&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;modifying-light-intensity&quot;&gt;Modifying light intensity&lt;/h3&gt;

&lt;p&gt;Adds the ability to change the intensity of light entities in the simulation at runtime.&lt;/p&gt;

&lt;p&gt;Pull request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/670&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/670&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/light_intensity.gif&quot; alt=&quot;Light intensity&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;joint-position-controller-topic-validity&quot;&gt;Joint Position Controller topic validity&lt;/h3&gt;

&lt;p&gt;Sanity checks for user provided topic values in joint controllers.&lt;/p&gt;

&lt;p&gt;Pull requests - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/632&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/632&lt;/a&gt; and &lt;a href=&quot;https://github.com/ignitionrobotics/ign-gazebo/pull/639&quot;&gt;https://github.com/ignitionrobotics/ign-gazebo/pull/639&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ignition-rviz&quot;&gt;Ignition RViz&lt;/h2&gt;

&lt;h3 id=&quot;tf-transform-status&quot;&gt;TF transform status&lt;/h3&gt;

&lt;p&gt;Adds the ability to show TF transform status in RViz.&lt;/p&gt;

&lt;p&gt;Pull request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rviz/pull/69&quot;&gt;https://github.com/ignitionrobotics/ign-rviz/pull/69&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TF Warn&lt;/th&gt;
      &lt;th&gt;TF Error&lt;/th&gt;
      &lt;th&gt;TF Valid&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/images/rviz_tf_warn.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/images/rviz_tf_error.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/images/rviz_tf_valid.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;bug-fix-in-rviz-framemanager&quot;&gt;Bug fix in RViz FrameManager&lt;/h3&gt;

&lt;p&gt;Fixes a bug where RViz won’t show entites in the absence of tf data.&lt;/p&gt;

&lt;p&gt;Pull request - &lt;a href=&quot;https://github.com/ignitionrobotics/ign-rviz/pull/67&quot;&gt;https://github.com/ignitionrobotics/ign-rviz/pull/67&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      

      
        <summary type="html">In this post I will be presenting my contributions to Open Robotics before joining them as a student developer. I contributed mainly to the Ignition Gazebo and Ignition RViz projects for a few months to get familiar with a large codebase.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Data Acquisition - DJS Racing</title>
      <link href="http://localhost:4000/djsr-daq" rel="alternate" type="text/html" title="Data Acquisition - DJS Racing" />
      <published>2021-07-31T15:48:00+05:30</published>
      <updated>2021-07-31T15:48:00+05:30</updated>
      <id>http://localhost:4000/djsr-daq</id>
      <content type="html" xml:base="http://localhost:4000/djsr-daq">&lt;div class=&quot;inner&quot;&gt;
    &lt;h1 class=&quot;major&quot;&gt;Data Acquisition for a Formula SAE car&lt;/h1&gt;
    &lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;assets/images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
    &lt;p&gt;At DJS Racing, the Formula Student team of Dwarkadas J. Sanghvi College of Engineering, we are building one of the first driverless formula student race-car. In this blog, we are presenting to you the user interface and analysis app of our data acquisition system for the year 2021.&lt;/p&gt;
    &lt;a href=&quot;https://github.com/djsracing/Nautilus&quot; target=&quot;_blank&quot;&gt;GitHub Repository&lt;/a&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 337.33px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image5.png&quot; style=&quot;width: 601.70px; height: 337.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Overview of our data acquisition system&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;This blog only deals with the cloud and software part of our system.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;To ensure that all of our subsystems work efficiently, we have developed our first-ever data visualization app. The app also supports Tableau integration for advanced data analysis and over-the-air (OTA) updates. We call the app &amp;ldquo;Nautilus&amp;rdquo;.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;h4&gt;&lt;span&gt;Key Features&lt;/span&gt;&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;span&gt;JavaScript and Node.js based backend for cross-platform support.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Fully featured dashboard complete with lap counter, track mapping, and vital stats bar graph.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Cloud-based duplex communication for analysis from home.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Race/session-based autosave feature in JSON format.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Plot mode for an interactive and clutter-free viewing experience.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Analysis mode for post-processing and visualization with the help of transport plots.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Users can rename sensors and change their mapping and units.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Users can save sensor data in CSV and PNG formats.&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span&gt;Kalman Filter for smoothing out GPS data.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;h4&gt;&lt;span&gt;Cloud infrastructure&lt;/span&gt;&lt;/h4&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 414.85px; height: 293.07px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image10.png&quot; style=&quot;width: 414.85px; height: 293.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Overview of our cloud infrastructure&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span class=&quot;c6 c7&quot;&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;The cloud framework is based on AWS EC2 compute platform and the flask micro web framework. socketIO is used to establish chat-rooms to relay sensor data in real-time. The data is also sent to Tableau client nodes through the Google Sheets API. Past data is logged to AWS S3 and PostgreSQL based database for backup. A replica of state-estimation, as discussed in &lt;a href=&quot;https://atharva-18.github.io/djsr.html&quot; target=&quot;_blank&quot;&gt;DJS Racing Driverless&lt;/a&gt; is implemented on the server-side. The estimated variables are then visualized in the analysis mode discussed below.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;h4&gt;&lt;span&gt;Desktop Application&lt;/span&gt;&lt;/h4&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 434.95px; height: 234.31px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image6.png&quot; style=&quot;width: 434.95px; height: 243.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Dashboard&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;The dashboard features a grid view of all sensors present on the car. Users can create and save sessions from the top panel. The track mapping from GPS data is created on this page.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 435.80px; height: 226.94px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image8.png&quot; style=&quot;width: 435.80px; height: 226.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Analysis&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;To debug any errors present in each subsystem, the app provides an offline visualization and debugging page called &amp;ldquo;Analysis&amp;rdquo;. Users can select sensors on task-based groups and can plot their graphs, lap-wise. Users are also presented with an option to load past data from a JSON file. Individual plots can be exported in CSV, SVG, and PNG file formats.&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 267.44px; height: 365.87px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image2.png&quot; style=&quot;width: 267.44px; height: 365.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c4 c15&quot;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 271.10px; height: 366.00px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image7.png&quot; style=&quot;width: 359.27px; height: 366.00px; margin-left: -42.32px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Connection Page &amp;amp; Track Mapping&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 294.51px; height: 250.13px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image9.png&quot; style=&quot;width: 542.88px; height: 320.01px; margin-left: -51.72px; margin-top: -48.74px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 291.50px; height: 251.15px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image4.png&quot; style=&quot;width: 308.95px; height: 251.15px; margin-left: -11.29px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Users can rename sensors and change their mapping from raw voltage&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;Often, team members need to change the mapping applied to the raw sensor voltages. Hence, we came up with a JavaScript arithmetic-based solution. Users can simply type in a JavaScript math expression and its unit for post-processing. This mapping configuration can be saved in a JSON file format to load variables instantly.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 443.75px; height: 237.50px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image1.png&quot; style=&quot;width: 443.75px; height: 237.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Plot Mode&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;Plot mode is provided for a clutter-free view of vital information&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 177.00px; height: 177.00px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image11.png&quot; style=&quot;width: 177.00px; height: 177.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span&gt;Each plot can be saved and exported in PNG, SVG, and CSV formats. &lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p align=&quot;center&quot;&gt;&lt;span style=&quot;overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 451.65px; height: 240.47px;&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;assets/images/image3.jpg&quot; style=&quot;width: 451.65px; height: 240.78px; margin-left: 0.00px; margin-top: -0.15px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;This was our first season where we developed our in-house data visualization software stack. This app will be used as a foundation for future support of Android and iOS.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span&gt;Experimental support for ROS (Robot Operating System) data visualization is also provided.&lt;/span&gt;&lt;/p&gt;
    &lt;p&gt;&lt;span class=&quot;c5 c12&quot;&gt;&lt;/span&gt;&lt;/p&gt;
  &lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      

      
        <summary type="html">Data Acquisition for a Formula SAE car At DJS Racing, the Formula Student team of Dwarkadas J. Sanghvi College of Engineering, we are building one of the first driverless formula student race-car. In this blog, we are presenting to you the user interface and analysis app of our data acquisition system for the year 2021. GitHub Repository Overview of our data acquisition system This blog only deals with the cloud and software part of our system. To ensure that all of our subsystems work efficiently, we have developed our first-ever data visualization app. The app also supports Tableau integration for advanced data analysis and over-the-air (OTA) updates. We call the app &amp;ldquo;Nautilus&amp;rdquo;. Key Features JavaScript and Node.js based backend for cross-platform support. Fully featured dashboard complete with lap counter, track mapping, and vital stats bar graph. Cloud-based duplex communication for analysis from home. Race/session-based autosave feature in JSON format. Plot mode for an interactive and clutter-free viewing experience. Analysis mode for post-processing and visualization with the help of transport plots. Users can rename sensors and change their mapping and units. Users can save sensor data in CSV and PNG formats. Kalman Filter for smoothing out GPS data. Cloud infrastructure Overview of our cloud infrastructure The cloud framework is based on AWS EC2 compute platform and the flask micro web framework. socketIO is used to establish chat-rooms to relay sensor data in real-time. The data is also sent to Tableau client nodes through the Google Sheets API. Past data is logged to AWS S3 and PostgreSQL based database for backup. A replica of state-estimation, as discussed in DJS Racing Driverless is implemented on the server-side. The estimated variables are then visualized in the analysis mode discussed below. Desktop Application Dashboard The dashboard features a grid view of all sensors present on the car. Users can create and save sessions from the top panel. The track mapping from GPS data is created on this page. Analysis To debug any errors present in each subsystem, the app provides an offline visualization and debugging page called &amp;ldquo;Analysis&amp;rdquo;. Users can select sensors on task-based groups and can plot their graphs, lap-wise. Users are also presented with an option to load past data from a JSON file. Individual plots can be exported in CSV, SVG, and PNG file formats. &amp;nbsp; &amp;nbsp; &amp;nbsp; Connection Page &amp;amp; Track Mapping Users can rename sensors and change their mapping from raw voltage Often, team members need to change the mapping applied to the raw sensor voltages. Hence, we came up with a JavaScript arithmetic-based solution. Users can simply type in a JavaScript math expression and its unit for post-processing. This mapping configuration can be saved in a JSON file format to load variables instantly. Plot Mode Plot mode is provided for a clutter-free view of vital information Each plot can be saved and exported in PNG, SVG, and CSV formats. This was our first season where we developed our in-house data visualization software stack. This app will be used as a foundation for future support of Android and iOS. Experimental support for ROS (Robot Operating System) data visualization is also provided.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Model Predictive Control</title>
      <link href="http://localhost:4000/mpcc" rel="alternate" type="text/html" title="Model Predictive Control" />
      <published>2021-07-31T15:48:00+05:30</published>
      <updated>2021-07-31T15:48:00+05:30</updated>
      <id>http://localhost:4000/mpcc</id>
      <content type="html" xml:base="http://localhost:4000/mpcc">&lt;section id=&quot;main&quot; class=&quot;wrapper&quot;&gt;
  &lt;div class=&quot;inner&quot;&gt;
  &lt;h1 class=&quot;major&quot;&gt;Contouring Model Predictive Control for RWD Race-cars&lt;/h1&gt;
    &lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
    &lt;p&gt;At DJS Racing, the Formula Student team of Dwarkadas J. Sanghvi College of Engineering, we are building one of the first driverless formula student race-car. In this presentation, we are presenting to you the improved control architecture of our autonomous system for the year 2021.&lt;/p&gt;
    &lt;p&gt;It is an extension of Contouring Model Predictive Control for RWD cars with a
    differential, using the HPIPM NLP solver. Globalrace-trajectory optimization
    for shortest time was done using Time-Optimal Trajectory Planning (Christ et
    al., 2019).&lt;/p&gt;
    &lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vQX6QqJKk9YsoXgC34c0_mbO6K9sN1kIp2U-qKnue3OFToLQhVGKgYfuYoX4LqNFLzPNYDtV5HJphwH/embed?start=false&amp;amp;loop=true&amp;amp;delayms=10000&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;749&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/section&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      

      
        <summary type="html">Contouring Model Predictive Control for RWD Race-cars At DJS Racing, the Formula Student team of Dwarkadas J. Sanghvi College of Engineering, we are building one of the first driverless formula student race-car. In this presentation, we are presenting to you the improved control architecture of our autonomous system for the year 2021. It is an extension of Contouring Model Predictive Control for RWD cars with a differential, using the HPIPM NLP solver. Globalrace-trajectory optimization for shortest time was done using Time-Optimal Trajectory Planning (Christ et al., 2019).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Monocular Depth Estimation</title>
      <link href="http://localhost:4000/cgan" rel="alternate" type="text/html" title="Monocular Depth Estimation" />
      <published>2021-07-31T15:48:00+05:30</published>
      <updated>2021-07-31T15:48:00+05:30</updated>
      <id>http://localhost:4000/cgan</id>
      <content type="html" xml:base="http://localhost:4000/cgan">&lt;section id=&quot;main&quot; class=&quot;wrapper&quot;&gt;
  &lt;div class=&quot;inner&quot;&gt;
  &lt;h1 class=&quot;major&quot;&gt;Monocular Depth Estimation using CGANs&lt;/h1&gt;
  &lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
  &lt;p&gt;Monocular depth estimation and object detection pipeline that makes use of Image-to-Image Translation with Conditional Adversarial Nets (Isola et al., 2017). The model was trained such that it learns the translation between a raw image and its stereo depth estimate.&lt;/p&gt;
  &lt;a href=&quot;https://github.com/atharva-18/Object-Streamer&quot; target=&quot;_blank&quot;&gt;GitHub Repository&lt;/a&gt;
  &lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vQw9TzhkEomSEa-RVDAG9pKhtg6seix5Y4w0NGu7vknCJWmcCrVuWLkrC9xOmck5v4tSByO3rbaXW-b/embed?start=false&amp;amp;loop=true&amp;amp;delayms=5000&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;749&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/section&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      

      
        <summary type="html">Monocular Depth Estimation using CGANs Monocular depth estimation and object detection pipeline that makes use of Image-to-Image Translation with Conditional Adversarial Nets (Isola et al., 2017). The model was trained such that it learns the translation between a raw image and its stereo depth estimate. GitHub Repository</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">DJS Racing</title>
      <link href="http://localhost:4000/djsr" rel="alternate" type="text/html" title="DJS Racing" />
      <published>2021-07-01T15:48:00+05:30</published>
      <updated>2021-07-01T15:48:00+05:30</updated>
      <id>http://localhost:4000/djsr</id>
      <content type="html" xml:base="http://localhost:4000/djsr">&lt;p&gt;I currently lead the driverless project at DJS Racing, a student team aiming to build a self-driving Formula Student race car.&lt;/p&gt;

&lt;p&gt;Here’s a small video from 2019 about our work prior to building driverless and electric cars:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/58pZoXDAzec&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Although we’ve been working remotely most of the times for the past one and half years, we’re on our way to finish building our next car by February 2022.&lt;/p&gt;

&lt;div id=&quot;wrapper&quot;&gt;

				&lt;!-- Main --&gt;
					&lt;section id=&quot;main&quot; class=&quot;wrapper&quot;&gt;
						&lt;div class=&quot;inner&quot;&gt;
							&lt;h1 class=&quot;major&quot;&gt;DJS Racing Driverless&lt;/h1&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/djsre03.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;assets/images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
							&lt;p&gt;At DJS Racing, the Formula Student Team of Dwarkadas J. Sanghvi College of Engineering, we are building one of the first driverless Formula Student race-car of India. The team will use the software system presented below for the Formula Student Driverless competition (postponed due to the outbreak of COVID-19).&lt;/p&gt;
							&lt;h2&gt;About Formula Student Driverless&lt;/h2&gt;
							&lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;assets/images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
							&lt;p&gt; Formula
								Student Driverless(FSD) is the world's largest autonomous racing competition, held annually in Hockenheim, Germany, in which multidisciplinary student teams compete
								with self-developed driverless racecars every year. 
								In this competition, each team has to race their autonomous car across 5 disciplines - Acceleration, Skidpad, Autocross, and Trackdrive. 
								In the main event, called Trackdrive, a car has to complete 10 laps on an unknown track autonomously as fast as possible against the clock. 
								The track is defined by a set of traffic cones, blue cones on the left and yellow cones on the right-hand side&lt;sup id=&quot;27_return&quot;&gt;&lt;a href=&quot;#27&quot;&gt;[27]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
								&lt;h4 align=&quot;center&quot;&gt;Track Layout&lt;/h4&gt;
								&lt;p align=&quot;center&quot;&gt;
									&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/track_layout.jpg&quot; /&gt;
								&lt;/p&gt;
								&lt;p&gt;In this blog, I am going to introduce to you the overall implementation of a Formula Student Driverless race-car by DJS Racing, India. The race-car can drive autonomously on unknown tracks and switch to Model Predictive Control after track exploration, mapping, and loop-closure detection in the first lap.&lt;/p&gt;
							&lt;h2&gt;Software Architecture&lt;/h2&gt;
							&lt;!-- &lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;assets/images/pic04.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt; --&gt;
							&lt;p&gt; We chose the Robot Operating System as our primary framework for software development. In order to run the MPC algorithm at its full potential, the track must be known at least 2s in advance for a speed of 85 kmph. With the current perception setup, it is not possible to achieve accurate mapping of the environment with a 40m (at 85 kmph) of look-ahead. Hence, we need to drive the first lap, exploring the track using a geometric controller such as the Pure Pursuit Controller&lt;sup id=&quot;23_return&quot;&gt;&lt;a href=&quot;#23&quot;&gt;[23]&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;28_return&quot;&gt;&lt;a href=&quot;#28&quot;&gt;[28]&lt;/a&gt;&lt;/sup&gt; at a lower speed. After successful mapping and exploration of the track, the car switches to the more performant Model Predictve Control(MPC) for the subsequent laps to race as fast as possible. From this, we can define two modes of our system operation, SLAM mode, and localization mode. To count the number of laps and to detect loop closure, high-level mission planning is done using a finite state machine. This state machine guides to the system to switch between SLAM and localization mode and signals whether the race has finished. Interfacing between the onboard Jetson AGX Xavier and the STM32 is done using the ros-serial package. The STM32 runs FreeRTOS to achieve real-time control of actuators.&lt;/p&gt;
							&lt;h4 align=&quot;center&quot;&gt;System Overview&lt;/h4&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/Autonomous System.png&quot; /&gt;
							&lt;/p&gt;
							&lt;h4 align=&quot;center&quot;&gt;High-level Architecture&lt;/h4&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/rosgraph.png&quot; /&gt;
							&lt;/p&gt;
							&lt;h2&gt;3D Perception&lt;/h2&gt;
							&lt;p&gt;The goal of the perception system is to detect traffic cones in the vicinity of the car and determine their color and 3D coordinates in real-time. These cone detections will be then fed as landmarks to the SLAM node to build a map of the track. The perception system takes input from multiple sensors - a mono camera, stereo camera, and a 3D LiDAR. The Basler Ace 2 industrial camera was chosen for our self-developed mono and stereo camera setups. &lt;/p&gt;
							&lt;!-- The Ouster OS1-32 LiDAR was chosen as our main LiDAR. --&gt;
							&lt;p&gt;Multiple sources of visual information are chosen so as to increase the redundancy&lt;sup id=&quot;22_return&quot;&gt;&lt;a href=&quot;#22&quot;&gt;[22]&lt;/a&gt;&lt;/sup&gt; in the system and to perceive the environment reliably as only one chance is given for the trackdrive race.&lt;/p&gt;

							&lt;h4&gt;LiDAR 3D Object Dectection&lt;/h4&gt;
							&lt;!-- Ouster OS-1 --&gt;
							&lt;p align=&quot;center&quot;&gt;
								3D Traffic Cone Detection and Color Estimation using LiDAR Data
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;video width=&quot;50%&quot; height=&quot;50%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
									&lt;source src=&quot;assets/videos/lidar.mp4&quot; type=&quot;video/mp4&quot; /&gt;
									Your browser does not support the video tag.
								&lt;/video&gt;
							&lt;/p&gt;
							&lt;p&gt;The point cloud obtained from the LiDAR is first segmented and the ground and out-of-track planes are removed. Afterwards, Euclidean clustering is run on the point cloud to cluster cone patches. The centroid of these cone patches is used to determine the 3D coordinates for each cone. They are further projected on to a 2D plane from the LiDAR frame so as to obtain a 32x32 image. A state-of-the-art CNN&lt;sup id=&quot;4_return&quot;&gt;&lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;&lt;/sup&gt; is then run on these image patches to estimate the color for each cone.&lt;/p&gt;
							&lt;p&gt;These clusters are also passed through a rule-based outlier filter to remove false cones in the map.&lt;/p&gt;
							&lt;h4&gt;CNN Architecture for Color Estimation using LiDAR Intensity Patterns&lt;/h4&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/lidar_cnn.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;h4&gt;Stereo 3D Object Dectection&lt;/h4&gt;
							&lt;p&gt;In order to ensure redundancy in the system, an additional camera-based object detection pipeline is added to the system.&lt;/p&gt;
							&lt;p&gt;We have chosen an off-the-shelf object detector, YOLOv3&lt;sup id=&quot;17_return&quot;&gt;&lt;a href=&quot;#17&quot;&gt;[17]&lt;/a&gt;&lt;/sup&gt; for object detection for real-time performance.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								Stereo Object Detection Pipeline
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/stereo_pipeline.png&quot; /&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								YOLOv3 running on mock cones
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
									&lt;source src=&quot;assets/videos/stereo.mp4&quot; type=&quot;video/mp4&quot; /&gt;
									Your browser does not support the video tag.
								&lt;/video&gt;
							&lt;/p&gt;
							&lt;p&gt;
								Stereo feature matching is then performed using the Semi-Global Block Matching&lt;sup id=&quot;32_return&quot;&gt;&lt;a href=&quot;#32&quot;&gt;[32]&lt;/a&gt;&lt;/sup&gt; (SGBM) algorithm to obtain the disparity map and 3D coordinates of the traffic cones.
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								3D point cloud obtained after occlusion reduction in the disparity map
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/stereo_depth.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;h4&gt;Monocular 3D Object Detection&lt;/h4&gt;
							&lt;p&gt;Monocular 3D Object Detection is performed using the Perspective 7-Point transformation with the PnP RANSAC algorithm&lt;sup id=&quot;1_return&quot;&gt;&lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;. This is considering the fact that we know the exact physical dimensions of each cone. The keypoints&lt;sup id=&quot;21_return&quot;&gt;&lt;a href=&quot;#21&quot;&gt;[21]&lt;/a&gt;&lt;/sup&gt; are then used to transform 2D image coordinates to 3D world coordinates.
								Keypoints for each cone image patch are obtained using the RektNet CNN architecture by MIT Driverless (Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions. Kieran Strobel et al., 2020).
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								Stereo Object Detection Pipeline
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/mono_pipeline.png&quot; /&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;35%&quot; height=&quot;35%&quot; src=&quot;assets/images/keypoint_input.png&quot; /&gt;
								&lt;img width=&quot;35%&quot; height=&quot;35%&quot; src=&quot;assets/images/keypoint_output.png&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Occlusion of background cones is handled by outlier filtering.&lt;/p&gt;
							&lt;h2&gt;
								Pose/Velocity Estimation and Simultaneous Localization and Mapping
							&lt;/h2&gt;
							&lt;p&gt;The extended Kalman Filter was chosen as an estimator for mildly non-linear systems with white Gaussian noise. Our main task here is to fuse data from an Inertial Measurement Unit (SBG Ellipse N), 4 wheel encoders, steering angle sensor, and a dual GPS system in a moving baseline configuration. EKF is also computationally more efficient than the Unscented Kalman Filter&lt;sup id=&quot;20_return&quot;&gt;&lt;a href=&quot;#20&quot;&gt;[20]&lt;/a&gt;&lt;/sup&gt; and allows us to estimate the state at 200 Hz. Here, the effects of lateral load transfer are ignored and the track is assumed to be a flat 2D surface.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								State Estimation Pipeline
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/velocity_estimation.png&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;We consider that our system model is a constant velocity model (CTRV)&lt;sup id=&quot;2_return&quot;&gt;&lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;, the jerk is assumed to be zero in this case and acceleration is used as a pseudo input to the system.&lt;/p&gt;
							&lt;p&gt;The wheel encoders along with the steering angle potentiometer are used to estimate the slip ratios of the vehicle.&lt;/p&gt;
							&lt;p&gt;The 6-DOF state vector is taken as&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;assets/images/state_vector.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Where &lt;code&gt;p, θ, v&lt;/code&gt; are the position (along x and y), angular velocity, and the linear velocity (along x and y in car frame) of the car respectively.&lt;/p&gt;
							&lt;p&gt;The process model is taken as&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;45%&quot; height=&quot;45%&quot; src=&quot;assets/images/process_model.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Where &lt;code&gt;a&lt;/code&gt; is the linear acceleration measured by the IMU and &lt;code&gt;R(θ)&lt;/code&gt; is the rotation matrix between the sensor frame and the vehicle frame.&lt;/p&gt;
							&lt;p&gt;The sensor model is&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;45%&quot; height=&quot;45%&quot; src=&quot;assets/images/sensor_model.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Where &lt;code&gt;θs&lt;/code&gt; is the sensor heading in the body frame and n{.} are additive Gaussian white noises that corrupt the measurements.&lt;/p&gt;
							&lt;p&gt;Different sensors work at different rates and the accuracy also varies between the sensors. Hence, we use multiple update functions for each sensor and we update our beliefs asynchronously&lt;sup id=&quot;26_return&quot;&gt;&lt;a href=&quot;#26&quot;&gt;[26]&lt;/a&gt;&lt;/sup&gt;. Here, the acceleration is taken from the IMU, angular velocity from the Gyroscope, and velocities from GPS and wheel encoders. Sensor drift and failure is detected with the help of Chi-Squared test&lt;sup id=&quot;7_return&quot;&gt;&lt;a href=&quot;#7&quot;&gt;[7]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img src=&quot;https://www.gstatic.com/education/formulas2/-1/en/chi_squared_test.svg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;If the value of Chi falls below a certain threshold, the measurement is ignored.&lt;/p&gt;
							&lt;p&gt;Design of an Autonomous Racecar: Perception, State Estimation and
								System Integration (Valls et al., 2018)
							&lt;/p&gt;
							&lt;p&gt;
								The landmark-based FastSLAM&lt;sup id=&quot;8_return&quot;&gt;&lt;a href=&quot;#8&quot;&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;15_return&quot;&gt;&lt;a href=&quot;#15&quot;&gt;[15]&lt;/a&gt;&lt;/sup&gt; algorithm was chosen for its robustness because of multiple-hypothesis and proposal sampling&lt;sup id=&quot;3_return&quot;&gt;&lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;31_return&quot;&gt;&lt;a href=&quot;#31&quot;&gt;[31]&lt;/a&gt;&lt;/sup&gt;.
								FastSLAM is computationally much more efficient with N.log(N) time complexity as compared to EKF SLAM with a quadratic complexity &lt;sup id=&quot;19_return&quot;&gt;&lt;a href=&quot;#19&quot;&gt;[19]&lt;/a&gt;&lt;/sup&gt;.
								FastSLAM is also much more tunable owing to the fact that it uses a Rao-Backwellized particle filter to express pose estimates. We can simply tune the number of particles to adjust the runtime performance.
								Data association is done by comparing the mahalanobis distance between an observation and each landmark on the map.
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/slam_pipeline.jpg&quot; /&gt;
							&lt;/p&gt;		
							&lt;p&gt;
								Our SLAM pipeline runs at 15 Hz to accommodate for the perception pipeline operation rate. The pose estimates from the SLAM node are then integrated by the velocity with time by the velocity estimation node to provide fast pose updates at 200 Hz for the control algorithm. This ensures that the MPC node always receives the latest pose estimates. 
							&lt;/p&gt;						
							&lt;h2&gt;
								Path Planning
							&lt;/h2&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;
									RRT + Delaunay Triangulation for Waypoints Generation
								&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p&gt;We chose the Randomly Exploring Random Trees&lt;sup id=&quot;24_return&quot;&gt;&lt;a href=&quot;#24&quot;&gt;[24]&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;25_return&quot;&gt;&lt;a href=&quot;#25&quot;&gt;[25]&lt;/a&gt;&lt;/sup&gt; as our path planning algorithms. Formula Student poses a unique challenge to path planning as there is no definite goal, and we just have to drive straight. Hence, in order to determine the best path, we have proposed a cost function that is assigned to each node. The tree branch with the lowest cost is then chosen as our desired path.&lt;/p&gt;
							&lt;code&gt;
								J = Q_delta * delta^2 + Q_theta * theta^2 + Q_cte * cte^2 - Q_cone * cone_count^2
							&lt;/code&gt;
							&lt;br /&gt;
							Where,
							&lt;ul&gt;
								&lt;li&gt;delta: Angular deviation between two nodes. Avoids sharp turns.&lt;/li&gt;
								&lt;li&gt;theta: Discounted heading. Tree pointing backward to the car are penalized.&lt;/li&gt;
								&lt;li&gt;cte: Distance between the path and track boundary. Penalizes branches too close to the boundaries.&lt;/li&gt;
								&lt;li&gt;cone_count: Count of cones on both sides of the path. Penalizes shorter paths.&lt;/li&gt;
							&lt;/ul&gt;
							&lt;p&gt;The cost is then normalized between 0 and 1 to determine the best path.&lt;/p&gt;
							&lt;p&gt;However, this is not the appropriate approach to path planning in FSD, as we need to have a continous and smooth trajectory in order to ensure that we are driving close to the limits of handling. Hence, we have an additional step of running Delaunay triangulation&lt;sup id=&quot;13_return&quot;&gt;&lt;a href=&quot;#13&quot;&gt;[13]&lt;/a&gt;&lt;/sup&gt; on nearby cone positions. The intersection points of the best tree branch and the traingulation segments are then taken as the waypoints for the control algorithm.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;video width=&quot;75%&quot; height=&quot;75%&quot; controls=&quot;&quot; loop=&quot;&quot;&gt;
									&lt;source src=&quot;assets/videos/rrt_delaunay.mp4&quot; type=&quot;video/mp4&quot; /&gt;
									Your browser does not support the video tag.
								&lt;/video&gt;
							&lt;/p&gt;
							&lt;p&gt;The waypoints that are generated are depicted as blue dots along the track center-line.&lt;/p&gt;
							&lt;h2&gt;Model Predictive Control&lt;/h2&gt;
							&lt;p&gt;After finishing the first lap using the PID and Pure Pursuit Controller, loop closure is detected and the SLAM algorithm switches to localization mode. This is all handled by the mission planning node. After track exploration and mapping, we switch to Model Predictve Control&lt;sup id=&quot;18_return&quot;&gt;&lt;a href=&quot;#18&quot;&gt;[18]&lt;/a&gt;&lt;/sup&gt; as our control method to finish the race as fast as possible at the limits of handling in real-time.&lt;/p&gt;
							&lt;p&gt;The goal of the optimization problem is to drive the car along the reference path, following imposed contraints and targeting the desired velocity. The state dynamics is considered as a bicycle model with actuator and drivetrain models along with tire forces.&lt;/p&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;Dynamic Bicycle Model&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;We consider the dynamic bicycle model for vehicle dynamics&lt;sup id=&quot;9_return&quot;&gt;&lt;a href=&quot;#9&quot;&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;14_return&quot;&gt;&lt;a href=&quot;#14&quot;&gt;[14]&lt;/a&gt;&lt;/sup&gt; with non-linear magic tire formula models&lt;sup id=&quot;16_return&quot;&gt;&lt;a href=&quot;#16&quot;&gt;[16]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;assets/images/dynamic_model.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;Vehicle Dynamics&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;assets/images/vehicle_dynamics.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Where &lt;i&gt;X, Y&lt;/i&gt; is the position and &lt;i&gt;&lt;/i&gt; is the orientation of the car in the world frame. &lt;i&gt;V&lt;sub&gt;x&lt;/sub&gt;, V&lt;sub&gt;y&lt;/sub&gt;&lt;/i&gt;, and &lt;i&gt;ω&lt;/i&gt; are the linear and angular velocities of the car in the body frame respectively. &lt;i&gt;𝛿&lt;/i&gt; is the steering angle, &lt;i&gt;L&lt;sub&gt;f&lt;/sub&gt;&lt;/i&gt; and &lt;i&gt;L&lt;sub&gt;r&lt;/sub&gt;&lt;/i&gt; are the length from the front and rear axles to the center-of-gravity respectively and &lt;i&gt;I&lt;sub&gt;z&lt;/sub&gt;&lt;/i&gt; is the inertia moment.&amp;lt;/i&amp;gt;&lt;/p&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;Tire Model&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;assets/images/tire_model.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;20%&quot; height=&quot;20%&quot; src=&quot;assets/images/tire_model_x.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;Here &lt;i&gt;D, C, B&lt;/i&gt; are coefficients obtained from the Pacejka tire model. &lt;i&gt;d&lt;/i&gt; is the driving command (acceleration), &lt;i&gt;C&lt;sub&gt;r&lt;/sub&gt;&lt;/i&gt; is the rolling resistance, &lt;i&gt;α&lt;sub&gt;f, r&lt;/sub&gt;&lt;/i&gt; are tire slip angles, and &lt;i&gt;C&lt;sub&gt;d&lt;/sub&gt;&lt;/i&gt; is the drag coefficient. &lt;i&gt;m&lt;/i&gt; is the mass of the vehicle.&lt;/p&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;State Input&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;25%&quot; height=&quot;25%&quot; src=&quot;assets/images/state_input.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;
								&lt;h4&gt;The lateral and cross-track errors are defined as&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p&gt;
								&lt;code&gt;
									cte` = cte - v * sin(epsi) * dt
								&lt;/code&gt;
								&lt;br /&gt;
								&lt;code&gt;
									epsi` = epsi +  v / Lf * (-delta) * dt
								&lt;/code&gt;
							&lt;/p&gt;
							&lt;p&gt;
								&lt;h4&gt;And hence, the cost function is defined as&lt;/h4&gt;
								&lt;code&gt;
									J = Q_cte * cte^2 + Q_epsi * epsi^2 + Q_v * (v - vmax)^2 + Q_delta * delta^2
								&lt;/code&gt;
								&lt;br /&gt;
								&lt;code&gt;
									+ Q_a * a^2 + F * (a` - a)^2 +  Q_ddelta * (delta` - delta)^2
								&lt;/code&gt;
							&lt;/p&gt;
							&lt;p&gt;The objective of this optimization problem is to drive the vehicle as close as possible to the track center line, with heading along the track direction. Rapid changes in the state inputs, as well as linear and angular velocities, are penalized.&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img src=&quot;assets/videos/mpc.gif&quot; width=&quot;75%&quot; height=&quot;75%&quot; alt=&quot;this slowpoke moves&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;
								The optimization problem is then solved directly using the IPOPT non-linear optimization library&lt;sup id=&quot;6_return&quot;&gt;&lt;a href=&quot;#6&quot;&gt;[6]&lt;/a&gt;&lt;/sup&gt; in a receding horizon fashion for over 15 steps for a horizon of 1.5 secs. There is an approximately 100 ms delay between computed command and actuation, hence we choose an actuator vector in the future (about 100 ms in the future) to compensate for mechanical delays.
								&lt;br /&gt;
							    However, an improvement is required here as IPOPT is not designed for real-time systems and considerably slows down with an increase in sequence size&lt;sup id=&quot;30_return&quot;&gt;&lt;a href=&quot;#30&quot;&gt;[30]&lt;/a&gt;&lt;/sup&gt;. Hence, we need to use a convex optimization library such as HPIPM&lt;sup id=&quot;29_return&quot;&gt;&lt;a href=&quot;#29&quot;&gt;[29]&lt;/a&gt;&lt;/sup&gt;. 
							&lt;/p&gt;
							&lt;h2&gt;Simulation and CI/CD Infrastructure&lt;/h2&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;Simulation Environment Designed with Gazebo&lt;sup id=&quot;12_return&quot;&gt;&lt;a href=&quot;#12&quot;&gt;[12]&lt;/a&gt;&lt;/sup&gt; and Ignition Libraries&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/simulation.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;
								&lt;h4 align=&quot;center&quot;&gt;ROS Bags from GitHub Pull Requests and actual car races can be visualized in the app&lt;/h4&gt;
							&lt;/p&gt;
							&lt;p align=&quot;center&quot;&gt;
								&lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;assets/images/image3.jpg&quot; /&gt;
							&lt;/p&gt;
							&lt;p&gt;The FSSIM &lt;sup id=&quot;4_return&quot;&gt;&lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;&lt;/sup&gt; simulation framework by AMZ Driverless has been used for the codebase. The FSSIM simulator employs a Gazebo plugin that uses a basic vehicle model which is discretized with Euler Forward discretization to achieve 99% of real-world performance. The simulation framework, on every pull request, runs the codebase and important ROS topics are recorded into ROS Bags and uploaded to the server. Each ROS Bag is played on the remote server and the run is recorded using VirtualGL, the videos are then stored in a NoSQL database for backup and querying. The client app can establish a remote database connection to visualize the ROS Bags. The server automatically replays the bag file and stores the recorded videos. To ensure software reliability, each pull request is first compiled using GitHub Actions and run on our CI server hosted on AWS. Only after all checks have passed and the simulation runs without any issues, a pull request can be merged.&lt;/p&gt;
							&lt;h4&gt;References&lt;/h4&gt;
							&lt;ol&gt;
								&lt;li id=&quot;1&quot;&gt;Kieran Strobel and Sibo Zhu and Raphael Chang and Skanda Koppula. Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions. 2020. arXiv:2007.13971&lt;sup&gt;&lt;/sup&gt;&lt;/li&gt;
								&lt;li id=&quot;2&quot;&gt;de la Iglesia Valls, M., Hendrikx, H. F. C., Reijgwart, V. J. F., Meier, F. V., Sa, I., Dub´e, R., Gawel, A.,
									B¨urki, M., and Siegwart, R. (2018). Design of an autonomous racecar: Perception, state estimation and
									system integration. 2018 IEEE International Conference on Robotics and Automation (ICRA).&lt;/li&gt;
								&lt;li id=&quot;3&quot;&gt;Dellaert, F., Fox, D., Burgard, W., and Thrun, S. (1999). Monte carlo localization for mobile robots. In
									IEEE International Conference on Robotics and Automation (ICRA99).&lt;/li&gt;
								&lt;li id=&quot;4&quot;&gt;Juraj Kabzan and Miguel de la Iglesia Valls and Victor Reijgwart and Hubertus Franciscus Cornelis Hendrikx and Claas Ehmke and Manish Prajapat and Andreas Bühler and Nikhil Gosala and Mehak Gupta and Ramya Sivanesan and Ankit Dhall and Eugenio Chisari and Napat Karnchanachari and Sonja Brits and Manuel Dangel and Inkyu Sa and Renaud Dubé and Abel Gawel and Mark Pfeiffer and Alexander Liniger and John Lygeros and Roland Siegwart.
									AMZ Driverless: The Full Autonomous Racing System. 2019. arXiv:1905.05150.
								&lt;/li&gt;
								&lt;li id=&quot;5&quot;&gt;Dhall, A., Dai, D., and Van Gool, L. (2019). Real-time 3D Traffic Cone Detection for Autonomous Driving.
									arXiv e-prints, page arXiv:1902.02394.
								&lt;/li&gt;
								&lt;li id=&quot;6&quot;&gt;
									Wächter, A., Biegler, L. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Math. Program. 106, 25–57 (2006). https://doi.org/10.1007/s10107-004-0559-y.
								&lt;/li&gt;
								&lt;li id=&quot;7&quot;&gt;
									B. D. Brumback and M. Srinath, “A chi-square test for fault-detection
									in kalman filters,” Automatic Control, IEEE Transactions on, vol. 32,
									pp. 552 – 554, 07 1987.
								&lt;/li&gt;
								&lt;li id=&quot;8&quot;&gt;
									M. Montemerlo, S. Thrun, D. Koller, B. Wegbreit, et al., “Fastslam:
									A factored solution to the simultaneous localization and mapping
									problem,” in Aaai/iaai, pp. 593–598, 2002.
								&lt;/li&gt;
								&lt;li id=&quot;9&quot;&gt;
									Kong, Jason, et al. &quot;Kinematic and dynamic vehicle models for
									autonomous driving control design.&quot; 2015 IEEE Intelligent Vehicles
									Symposium (IV). IEEE, 2015.
								&lt;/li&gt;
								&lt;li id=&quot;10&quot;&gt;
									Yoon, Jong-Hwa, and Huei Peng. &quot;A cost-effective sideslip
									estimation method using velocity measurements from two GPS
									receivers.&quot; IEEE Transactions on Vehicular Technology 63.6
									(2013): 2589-2599.
								&lt;/li&gt;
								&lt;li id=&quot;11&quot;&gt;
									Klomp, M., Olsson, K., and Sandberg, C. (2014). Non-linear steering control for limit handling conditions
									using preview path curvature. International Journal of Vehicle Autonomous Systems, 12(3):266–283.
								&lt;/li&gt;
								&lt;li id=&quot;12&quot;&gt;
									Koenig, N. and Howard, A. (2004). Design and use paradigms for gazebo, an open-source multi-robot simulator. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International
									Conference on, volume 3, pages 2149–2154. IEEE.
								&lt;/li&gt;
								&lt;li id=&quot;13&quot;&gt;
									Lee and Schachter (1980). Two algorithms for constructing a delaunay triangulation. International Journal
									of Computer and Information Sciences.
								&lt;/li&gt;
								&lt;li id=&quot;14&quot;&gt;
									Milliken, W., Milliken, D., and of Automotive Engineers, S. (1995). Race Car Vehicle Dynamics. Premiere
									Series. SAE International.
								&lt;/li&gt;
								&lt;li id=&quot;15&quot;&gt;
									Montemerlo, M., Thrun, S., Koller, D., and Wegbreit, B. (2003). FastSLAM 2.0 : An Improved Particle
									Filtering Algorithm for Simultaneous Localization and Mapping that Provably Converges. In Proceedings
									of the 18th international joint conference on Artificial intelligence, pages 1151–1156.
								&lt;/li&gt;
								&lt;li id=&quot;16&quot;&gt;
									Pacejka, H. B. and Bakker, E. (1992). The magic formula tyre model. Vehicle system dynamics, 21(S1):1–18.
								&lt;/li&gt;
								&lt;li id=&quot;17&quot;&gt;
									Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You only look once: Unified, real-time object
									detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
									779–788.
								&lt;/li&gt;
								&lt;li id=&quot;18&quot;&gt;
									Rosolia, U., Carvalho, A., and Borrelli, F. (2017). Autonomous racing using learning model predictive
									control. In American Control Conference (ACC), pages 5115–5120. IEEE.
								&lt;/li&gt;
								&lt;li id=&quot;19&quot;&gt;
									Thrun, S., Burgard, W., and Fox, D. (2005). Probabilistic Robotics (Intelligent Robotics and Autonomous
									Agents). The MIT Press.
								&lt;/li&gt;
								&lt;li id=&quot;20&quot;&gt;
									Xue, Z. and Schwartz, H. (2013). A comparison of several nonlinear filters for mobile robot pose estimation.
									In 2013 IEEE International Conference on Mechatronics and Automation.
								&lt;/li&gt;
								&lt;li id=&quot;21&quot;&gt;
									Tulsiani, S. and Malik, J. (2015). Viewpoints and keypoints. In 2015 IEEE Conference on Computer Vision
									and Pattern Recognition (CVPR).
								&lt;/li&gt;
								&lt;li id=&quot;22&quot;&gt;
									Gosala, N. B., B¨uhler, A., Prajapat, M., Ehmke, C., Gupta, M., Sivanesan, R., Gawel, A., Pfeiffer, M.,
									B¨urki, M., Sa, I., Dub´e, R., and Siegwart, R. (2018). Redundant Perception and State Estimation for
									Reliable Autonomous Racing. ArXiv e-prints.
								&lt;/li&gt;
								&lt;li id=&quot;23&quot;&gt;
									Coulter, R. C. (1992). Implementation of the pure pursuit path tracking algorithm. Technical report,
									Carnegie-Mellon UNIV Pittsburgh PA Robotics INST.
								&lt;/li&gt;
								&lt;li id=&quot;24&quot;&gt;
									LaValle, Steven M. (October 1998). &quot;Rapidly-exploring random trees: A new tool for path planning&quot; (PDF). Technical Report. Computer Science Department, Iowa State University (TR 98–11).
								&lt;/li&gt;
								&lt;li id=&quot;25&quot;&gt;
									Howie Choset, James Kuffner. &quot;Robotic Motion Planning: RRT’s&quot;. Robotic Motion Planning, 16-735. The Robotics Institute, Carnegie Mellon University.
								&lt;/li&gt;
								&lt;li id=&quot;26&quot;&gt;
									K. Hausman, S. Weiss, R. Brockers, L. Matthies, and G. S. Sukhatme,
“Self-calibrating multi-sensor fusion with probabilistic measurement
validation for seamless sensor switching on a uav,” in 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 4289–
4296, May 2016.
								&lt;/li&gt;
								&lt;li id=&quot;27&quot;&gt;
									Formula Student Rules 2020 v1.0. https://www.formulastudent.de/fileadmin/user_upload/all/2020/rules/FS-Rules_2020_V1.0.pdf
								&lt;/li&gt;
								&lt;li id=&quot;28&quot;&gt;
									J. Ni and J. Hu, &quot;Path following control for autonomous formula racecar: Autonomous formula student competition,&quot; 2017 IEEE Intelligent Vehicles Symposium (IV), Los Angeles, CA, 2017, pp. 1835-1840, doi: 10.1109/IVS.2017.7995972.
								&lt;/li&gt;
								&lt;li id=&quot;29&quot;&gt;
									G. Frison, M. Diehl. HPIPM: a high-performance quadratic programming framework for model predictive control. 2020. arXiv preprint, arXiv:2003.02547
								&lt;/li&gt;
								&lt;li id=&quot;30&quot;&gt;
									G. Frison, D. K. M. Kufoalor, L. Imsland and J. B. Jørgensen, &quot;Efficient implementation of solvers for linear model predictive control on embedded devices,&quot; 2014 IEEE Conference on Control Applications (CCA), Juan Les Antibes, 2014, pp. 1954-1959, doi: 10.1109/CCA.2014.6981589.
								&lt;/li&gt;
								&lt;li id=&quot;31&quot;&gt;
									J. D. Hol, T. B. Schon, and F. Gustafsson, “On resampling algorithms
for particle filters,” in Nonlinear Statistical Signal Processing Workshop, 2006 IEEE, pp. 79–82, IEEE, 2006.
								&lt;/li&gt;
								&lt;li id=&quot;32&quot;&gt;
									SGBM, by Hirschmuller, H. (2008). Stereo processing by semiglobal matching and mutual information. IEEE Transactions on pattern analysis and machine intelligence, 30(2), 328-341.
								&lt;/li&gt;
							&lt;/ol&gt;
						&lt;/div&gt;
					&lt;/section&gt;
			&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Atharva Pusalkar</name>
        
        
      </author>

      

      
        <category term="Projects" />
      
        <category term="Experiences" />
      

      
        <summary type="html">I currently lead the driverless project at DJS Racing, a student team aiming to build a self-driving Formula Student race car.</summary>
      

      
      
    </entry>
  
</feed>
